# Chapter 9: Consistency and Consensus

## Introduction

### Welcome to the Hard Parts

Congratulations on making it to Chapter 9! If you're reading this, you've already covered a lot of ground. In the previous chapters, we explored:

- **Chapter 1-2**: Single-node database fundamentals and data models
- **Chapter 3**: Storage structures like B-trees and LSM-trees  
- **Chapter 4**: Encoding and schema evolution
- **Chapter 5**: Replication strategies (leader-follower, multi-leader, leaderless)
- **Chapter 6**: Partitioning and sharding
- **Chapter 7**: Transactions and isolation levels
- **Chapter 8**: Distributed systems challenges (network failures, clocks, process pauses)

Now we're diving into one of the **most theoretically dense but practically important** chapters in the book. This chapter gets a bit heavy on theoryâ€”there are algorithms, proofs, and abstract concepts that might feel dry compared to earlier chapters. But stick with it! These concepts are the foundation of **every reliable distributed system** you use daily: Google's Spanner, Amazon's DynamoDB, Apache Kafka, Kubernetes etcd, and more.

### What Makes This Chapter Challenging?

This chapter is theoretical because it deals with **fundamental impossibilities and trade-offs** in distributed systems. We're not just learning "how to do X in database Y"â€”we're learning **what's physically possible** and what isn't when data lives on multiple machines connected by unreliable networks.

Think of it this way: If you're building a house, earlier chapters taught you about different types of wood, nails, and tools. This chapter teaches you about **gravity and physics**â€”the fundamental laws you must work within, no matter how skilled you are.

### The Core Challenge: Agreement in Distributed Systems

In distributed systems, achieving agreement among nodes is one of the hardest problems. This chapter explores the challenges and solutions for ensuring consistency and reaching consensus in the face of:
- **Network delays and partitions**: Messages can be delayed, duplicated, or lost entirely
- **Node failures**: Servers crash, restart, or lose power at any moment  
- **Clock skew**: No two machines agree on what time it is
- **Concurrent operations**: Multiple operations happen simultaneously across different nodes

We'll cover fundamental concepts:
- **Linearizability**: The strongest consistency guarantee (makes system behave as if there's only one copy of data)
- **Ordering guarantees**: Causal consistency and total order (tracking cause-and-effect relationships)
- **Distributed transactions**: Two-phase commit (2PC) and atomic operations across multiple nodes
- **Consensus algorithms**: Paxos, Raft, and ZooKeeper (how nodes agree despite failures)
- **Membership and coordination**: How nodes agree on cluster state and who's alive

### Why This Matters

Understanding consensus is essential for:

1. **Building Correct Systems**: If you're building a distributed system (microservices, databases, message queues), you need to know what guarantees are possible and what aren't.

2. **Debugging Production Issues**: When your distributed system behaves weirdly (data appears then disappears, writes get lost, reads return stale data), this chapter helps you understand **why** and **how to fix it**.

3. **Making Informed Trade-offs**: Every distributed system makes trade-offs between consistency, availability, and performance. You can't make good decisions without understanding these concepts.

4. **Avoiding Costly Mistakes**: Incorrect assumptions about distributed systems lead to data loss, corruption, and angry customers. Many startups have failed because they didn't understand these principles.

### A Note for Beginners

If this is your first exposure to distributed systems theory, don't worry if you don't understand everything on the first read. These are genuinely hard problems that took decades of research to solve. The goal isn't to memorize every algorithmâ€”it's to develop **intuition** about:

- When can you trust your data?
- What happens when things fail?
- Which problems can be solved and which can't?

Let's dive in! We'll start with one of the most important concepts: **linearizability**.

---

## Part 1: Consistency Guarantees

### What is Consistency?

**Consistency** defines what guarantees a system provides about the state of data, especially during concurrent operations and failures.

**Spectrum of Consistency Models:**

```
Stronger (Harder to scale) â†â†’ Weaker (Easier to scale)

Linearizability
    â”‚
Causal Consistency
    â”‚
Sequential Consistency
    â”‚
Eventual Consistency
    â”‚
No Guarantees
```

### The Replication Lag Problem

In Chapter 5, we saw that replication lag causes temporary inconsistencies:

```
Write to Leader:
  t=0: SET user_123 = "Alice"

Followers:
  t=0:   Follower 1: user_123 = "Bob" (old value)
  t=0:   Follower 2: user_123 = "Bob" (old value)
  
  t=10ms: Follower 1: user_123 = "Alice" (updated)
  t=50ms: Follower 2: user_123 = "Alice" (updated)

Read from Follower 1 at t=5ms  â†’ Returns "Bob"  (stale read!)
Read from Follower 2 at t=30ms â†’ Returns "Bob"  (stale read!)
Read from Leader at t=0ms      â†’ Returns "Alice" (up-to-date)
```

**Questions:**
- How can we provide stronger guarantees?
- What does it mean for a distributed system to "behave correctly"?

## Part 2: Linearizability - Making Distributed Systems Act Like One Copy

### Understanding the Core Problem

Imagine you're watching a big sports game, like the World Cup final. The game ends, Germany wins, and the final score is recorded. You refresh ESPN.com on your phoneâ€”you see Germany won! Your friend sitting right next to you refreshes their phone at the same momentâ€”they see the game is still in progress!

**What just happened?**

This isn't a bug in ESPN's code. This is a fundamental challenge of **distributed systems with replication**. ESPN (like most high-traffic websites) doesn't have just one database server. They have:
- One **leader** database (where writes go)
- Multiple **follower** (replica) databases (where reads come from)

When the final score is written to the leader, it takes time to **replicate** to all the followers. During that replication lag:
- Some users see the new score (if their request hits an updated replica)
- Other users see the old score (if their request hits a stale replica)

This is called a **replication lag** problem, and it violates something called **linearizability**.

### Definition of Linearizability

**Linearizability** (also called atomic consistency or strong consistency) is the strongest consistency guarantee possible:

> Once a write completes and is acknowledged, ALL subsequent reads (from any client, to any node) must return that value or a newer one. The system must behave as if there's only ONE copy of the data, and all operations happen instantaneously in some definite order.

**Intuition for Beginners**: 

Think of a single-player video game where you save your progress. Every time you load the game, you always see your latest saveâ€”never an older one. Linearizability makes distributed systems behave the same way, even though data is copied across multiple machines.

**Why it's called "Linearizability"**: 

All operations can be arranged in a single **linear** sequence (like beads on a string), where each operation takes effect atomically at some point between its invocation and response. No going backwards, no contradictions.

### The ESPN World Cup Example (Detailed)

Let's dive deep into a real-world scenario that shows why linearizability is hard and why it matters.

**The Setup:**
- **ESPN.com** powers real-time sports scores for millions of users
- They use a **leader-follower replication** setup:
  - 1 Leader database (handles all writes)
  - 2 Follower databases (handle reads, located in different data centers)
- **Alice and Bob** are watching the World Cup final
- The game just ended: **Germany wins 1-0**

**Timeline of Events (Non-Linearizable System):**

```
t=0ms: Referee enters final score into system
       â†“
       [WRITE to Leader: game_id=2023_final, status="FINISHED", winner="Germany"]
       â†“
       Leader acknowledges: "Write successful!"
       
t=1ms: Leader starts replicating to followers
       â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   LEADER        â”‚ status="FINISHED", winner="Germany" âœ“
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Follower1â”‚   â”‚Follower2â”‚
    â”‚         â”‚   â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
t=10ms: Follower 1 receives update (fast network path)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚Follower1â”‚ status="FINISHED", winner="Germany" âœ“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
t=10ms: Alice refreshes ESPN.com
        â†“
        [Her request routes to Follower 1]
        â†“
        Alice sees: "GAME OVER - Germany wins 1-0!" âœ“

t=11ms: Bob refreshes ESPN.com (sitting next to Alice!)
        â†“
        [His request routes to Follower 2]
        â†“
        Follower 2 STILL has old data (slow network)
        â†“
        Bob sees: "GAME IN PROGRESS - Germany 0 - Brazil 0" âœ—

t=100ms: Follower 2 finally receives update
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚Follower2â”‚ status="FINISHED", winner="Germany" âœ“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

t=101ms: Bob refreshes again
         â†“
         Now Bob sees: "GAME OVER - Germany wins 1-0!" âœ“
```

**The Problem:**
- Alice and Bob are in the **same room**
- Alice saw the final score **10ms after it was written**
- Bob, refreshing **1ms later**, saw **stale data**
- This violates our intuition: Bob's read happened **after** the write was acknowledged, but he didn't see it!

**Why This is Bad:**
1. **Confusing User Experience**: Bob sees the game still in progress after Alice announces Germany won
2. **Coordination Issues**: Imagine this is financial data (account balance), not sports scores
3. **No Clear "Truth"**: Different users see different realities at the same moment

This system is **NOT linearizable** because:
- Alice's read returned the new value
- Bob's later read returned an old value  
- Values went "backwards in time"

### What Would Linearizability Look Like?

In a **linearizable system**, once the leader acknowledges the write (at t=1ms), **every single read** from t=1ms onwards would return the new value, regardless of which replica handles the request.

**Linearizable Timeline:**

```
t=0ms: Write to Leader
t=1ms: Leader acknowledges âœ“
       â†“
       From this point forward, ALL reads see new value
       â†“
t=10ms: Alice reads â†’ "Germany wins" âœ“
t=11ms: Bob reads â†’ "Germany wins" âœ“
t=50ms: Charlie reads â†’ "Germany wins" âœ“

NO ONE can see old value after t=1ms
```

**How to Achieve This:**

We have a few options (we'll explore each in detail):

1. **Only read from leader** (simple but doesn't scale)
2. **Synchronous replication** (wait for all replicas before acknowledging write)
3. **Read-your-writes consistency** (session-based routing)
4. **Consensus algorithms** (Raft, Paxosâ€”complex but correct)

Each option has trade-offs in performance, availability, and complexity.

### Linearizability Example

**Scenario**: Alice and Bob are both viewing a user profile.

**Timeline:**

```
Alice's operations:                Bob's operations:
    â”‚                                  â”‚
t=0 â”‚ WRITE(user_123, "New Name")     â”‚
    â”‚ (acknowledged)                   â”‚
    â”‚                                  â”‚
t=1 â”‚                                  â”‚ READ(user_123)
    â”‚                                  â”‚ â†’ Must return "New Name"
    â”‚                                  â”‚    (not "Old Name")
```

**Linearizability guarantees**:
1. Once write is acknowledged, all reads see new value
2. Total ordering of operations exists
3. Operations appear to happen atomically

**Visual: Linearizable System**

```
Time â†’
Client 1: â”€â”€WRITE(x=1)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                â”‚
                âœ“ (acknowledged)
                â”‚
Client 2: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€READ(x)â†’1â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         â”‚
                         â””â”€ Must see new value!
```
Problem**: Reading only from leader creates a bottleneck. The whole point of replicas is to distribute read load!

**2. Consensus Algorithms (linearizable)**
- Raft, Paxos, ZooKeeper
- Coordinate writes across multiple nodes
- More expensive but truly linearizable

**3. Multi-Leader Replication (NOT linearizable)**
- Concurrent writes to different leaders
- Conflicts resolved asynchronously

**4. Leaderless Replication with Quorums (NOT necessarily linearizable)**
- Even with strict quorums (w + r > n)
- Concurrent writes and network delays cause issues

### Semi-Synchronous Replication: The Practical Middle Ground

In the ESPN example, we saw two extremes:
1. **Fully asynchronous replication**: Leader acknowledges write immediately, replicas updated later (fast but not linearizable)
2. **Fully synchronous replication**: Leader waits for ALL replicas before acknowledging (linearizable but slow and fragile)

In practice, most production systems use **semi-synchronous replication**â€”a hybrid approach that balances:
- **Performance**: Don't wait too long
- **Durability**: Data exists on multiple machines  
- **Availability**: System tolerates some failures

#### What is Semi-Synchronous Replication?

**Semi-synchronous replication** means:
> The leader waits for acknowledgment from **some** (but not all) replicas before confirming the write to the client.

**Configuration Options:**

```javascript
class SemiSyncConfig {
    constructor(totalReplicas, requiredAcks) {
        this.totalReplicas = totalReplicas;  // Total number of replicas
        this.requiredAcks = requiredAcks;    // How many must acknowledge
    }
    
    isDurable() {
        // Check if write is durable
        return this.requiredAcks >= 2;  // At least 2 copies (leader + 1 replica)
    }
    
    canTolerateFailures() {
        // How many replicas can fail without data loss
        return this.requiredAcks - 1;
    }
}

// Example configurations:
const config1 = new SemiSyncConfig(3, 2);
// Wait for leader + 1 replica (can tolerate 1 failure)

const config2 = new SemiSyncConfig(5, 3);
// Wait for leader + 2 replicas (can tolerate 2 failures)
```

#### Visual: Semi-Synchronous Replication

**Scenario**: 1 leader + 2 replicas, require 2 acknowledgments (leader + 1 replica)

```
Client: "Write score: Germany wins"
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEADER  â”‚ â† Receives write
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”‚ Replication starts immediately to ALL replicas
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Replica 1â”‚    â”‚Replica 2â”‚    â”‚Replica 3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                â”‚                âš¡ (network slow/partition)
   â”‚                â”‚                
   â–¼                â–¼                
[ACK] 5ms      [ACK] 20ms       â³ (still waiting...)
   â”‚                â”‚                
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                
            â–¼                        
       Got 2 ACKs!                   
       (required threshold met)      
            â”‚                        
            â–¼                        
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              
   â”‚ Client: SUCCESS â”‚              
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              
   
   Replica 3 eventually catches up (or gets replaced if failed)
```

**Key Points:**
1. Leader doesn't wait for Replica 3 (which might be slow/failed)
2. Client gets fast response (5-20ms instead of timeout)
3. Data is durable (exists on 2 machines)
4. System remains available even if 1 replica is down

#### Why Semi-Sync is a Sweet Spot

Let's compare with concrete numbers:

**Performance Comparison:**

```javascript
// Simulation results from typical datacenter

// Fully Async (no waiting)
const asyncLatency = "1ms";   // Just write to leader
const asyncDurability = "1 copy";  // Only on leader initially
const asyncFailureTolerance = "0 replicas can fail without data loss";

// Semi-Sync (wait for 1 replica)
const semiSyncLatency = "5-10ms";  // Wait for fastest replica
const semiSyncDurability = "2 copies";  // Leader + 1 replica
const semiSyncFailureTolerance = "1 replica can fail";

// Fully Sync (wait for ALL replicas)
const fullSyncLatency = "100-500ms";  // Wait for slowest replica
const fullSyncDurability = "4 copies";  // Leader + all replicas
const fullSyncFailureTolerance = "3 replicas can fail";
```

**Why 100-500ms for Fully Sync?**

In a geographically distributed system:
- Replica in same datacenter: 1-5ms
- Replica in nearby city: 10-20ms  
- Replica across country: 50-100ms
- Replica on different continent: 100-300ms

If you wait for the **slowest** replica (e.g., across the world), plus potential network congestion, packet loss, retries... you could easily wait 500ms or more. **That's unacceptable for interactive applications!**

#### Real-World Configuration Examples

**MySQL Semi-Synchronous Replication:**

```sql
-- Enable semi-sync on leader
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
SET GLOBAL rpl_semi_sync_master_enabled = 1;

-- Wait for at least 1 replica
SET GLOBAL rpl_semi_sync_master_wait_for_slave_count = 1;

-- Timeout if no replicas respond in 10 seconds
SET GLOBAL rpl_semi_sync_master_timeout = 10000;
```

**PostgreSQL Synchronous Replication:**

```
# postgresql.conf

# Synchronous commit levels:
synchronous_commit = 'on'          # Wait for WAL write + flush
synchronous_commit = 'remote_apply' # Wait for replica to apply

# List of synchronous standbys (require 2 acknowledgments)
synchronous_standby_names = 'FIRST 2 (replica1, replica2, replica3)'
```

**MongoDB Write Concerns:**

```javascript
const { MongoClient } = require('mongodb');

const client = new MongoClient();
const db = client.db('mydb');

// Write concern: Require majority of replicas
const result = await db.collection('scores').insertOne(
    { gameId: "2023_final", winner: "Germany" },
    { writeConcern: { w: "majority", j: true } }
);

// w: 1 - Leader only (async)
// w: 2 - Leader + 1 replica
// w: "majority" - Majority of replica set (best for durability)
// j: true - Wait for journal flush (survives crash)
```

#### When to Use Each Strategy

| Use Case | Recommended Strategy | Reasoning |
|----------|---------------------|-----------|
| **Financial transactions** | Fully Synchronous or Semi-Sync (majority) | Can't afford data loss |
| **User-generated content** | Semi-Sync (1-2 replicas) | Balance speed and durability |
| **Analytics/Metrics** | Fully Async | Speed matters, occasional loss acceptable |
| **Session data** | Semi-Sync (1 replica) | Important but not critical |
| **Cache/Temporary data** | Fully Async | Can be regenerated |

#### The Hidden Cost: Locks and Blocking

There's a subtle but critical issue with synchronous replication we haven't discussed yet: **locks**.

**The Lock Problem:**

```
Time  Leader              Replica 1           Replica 2
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t=0ms â”‚ Receive write     â”‚                  â”‚
      â”‚ BEGIN transaction â”‚                  â”‚
      â”‚ LOCK row 123      â”‚                  â”‚  
      â”‚                   â”‚                  â”‚
t=1ms â”‚ Send to replicasâ”€>â”‚ Receive          â”‚ Receive
      â”‚                   â”‚ BEGIN txn        â”‚ BEGIN txn
      â”‚                   â”‚ LOCK row 123     â”‚ LOCK row 123
      â”‚                   â”‚                  â”‚
t=5ms â”‚ Got ACK from R1  <â”€â”˜                 â”‚
      â”‚ Still waiting...  â”‚                  â”‚âš¡ Network issue!
      â”‚                   â”‚                  â”‚
t=10s â”‚ ğŸ“Œ STILL LOCKED!  â”‚                  â”‚ ğŸ“Œ STILL LOCKED!
      â”‚ Other queries     â”‚                  â”‚ Other queries
      â”‚ waiting on row123 â”‚                  â”‚ waiting on row123
      â”‚                   â”‚                  â”‚
t=30s â”‚ TIMEOUT!          â”‚                  â”‚ Finally responds!
      â”‚ Unlock, ROLLBACK  â”‚                  â”‚
```

**What just happened:**

1. Client sends write to leader
2. Leader locks row 123 and starts transaction
3. Leader sends data to Replica 1 and Replica 2
4. Replica 1 responds quickly (5ms)  
5. Replica 2 has network issues, takes 30 seconds
6. **For 30 seconds**, row 123 is LOCKED on leader and both replicas
7. **Any other client** trying to read/write row 123 must wait 30 seconds
8. This creates a **cascading backup** of queries

**This is why synchronous replication can be dangerous in production!**

One slow replica can lock up your entire database, causing timeouts for all users trying to access that data.

**How Semi-Sync Helps:**

With semi-sync (requiring only 1 replica ACK), if Replica 2 is slow:
- Leader + Replica 1 quickly confirm (5ms)
- Locks released after 5ms
- Other queries proceed normally
- Replica 2 catches up asynchronously

This is the **primary reason** semi-sync is the recommended approach for most production systems.
Client 1 writes to Node A: x = 1
Client 2 writes to Node B: x = 2

Client 3 reads from Node A: x = 1
Client 4 reads from Node B: x = 2

This violates linearizability!
(No total order exists)
```

### What Makes a System Linearizable?

**Linearizability requires:**
1. **Recency guarantee**: Read returns most recent write
2. **Global ordering**: All operations have a total order
3. **Real-time ordering**: If op1 completes before op2 starts, op1 < op2 in total order

**Non-linearizable example:**

```python
# Thread 1
database.write('x', 1)
database.write('x', 2)

# Thread 2Understanding Transactions First (A Beginner's Guide)

Before we tackle **distributed transactions** (transactions spanning multiple databases), we need to make sure everyone understands **regular transactions** on a single database. This might seem basic, but the transcript discussion revealed that many developers don't fully grasp these fundamentals.

### What is a Transaction?

A **transaction** is a group of database operations that should be treated as a **single atomic unit**â€”either ALL operations succeed, or ALL fail (none take effect).

**Real-World Analogy:**

Imagine ordering food delivery:
1. Charge your credit card
2. Create order in restaurant system
3. Assign driver
4. Update inventory

If step 2 fails (restaurant system is down), you don't want:
- Your card still charged âŒ
- No order created âŒ  
- Driver assigned to nothing âŒ

You want ALL steps to fail togetherâ€”**atomicity**.

### Basic Transaction Syntax

**Simple Query (Auto-committed):**

```sql
-- This is automatically wrapped in a transaction
SELECT * FROM users WHERE id = 10;

-- Equivalent to:
BEGIN TRANSACTION;
    SELECT * FROM users WHERE id = 10;
COMMIT;
```

**Manual Transaction:**

```sql
BEGIN TRANSACTION;
    
    -- Multiple operations
    SELECT balance FROM accounts WHERE id = 123;
    
    UPDATE accounts 
    SET balance = balance - 100 
    WHERE id = 123;
    
    UPDATE accounts 
    SET balance = balance + 100 
    WHERE id = 456;
    
    INSERT INTO transaction_log (from_account, to_account, amount)
    VALUES (123, 456, 100);
    
COMMIT;  -- All changes take effect together
```

**Rolling Back (Undoing Changes):**

```sql
BEGIN TRANSACTION;
    
    UPDATE accounts SET balance = balance - 100 WHERE id = 123;
    
    -- Oh no, something went wrong!
    SELECT balance FROM accounts WHERE id = 456;
    -- Returns NULL - account doesn't exist!
    
ROLLBACK;  -- Undo everything in this transaction

-- Account 123 balance is unchanged (rollback undid the UPDATE)
```

### Autocommit: What It Means

**Autocommit** is a database setting that controls whether single statements are automatically wrapped in transactions.

**With Autocommit ON (default in most databases):**

```sql
-- Each statement is its own transaction
INSERT INTO users (name) VALUES ('Alice');  -- Committed immediately
INSERT INTO users (name) VALUES ('Bob');    -- Committed immediately

-- If second INSERT fails, first INSERT still succeeded
```

**With Autocommit OFF:**

```sql
-- Must explicitly BEGIN and COMMIT
INSERT INTO users (name) VALUES ('Alice');  -- NOT committed yet
INSERT INTO users (name) VALUES ('Bob');    -- NOT committed yet
-- ... application crashes ...
-- Both inserts are LOST (never committed)
```

**Why Autocommit ON is Usually Better:**

```javascript
// JavaScript example with node-postgres (PostgreSQL)

const { Client } = require('pg');

const client = new Client({
    database: 'mydb',
    user: 'postgres'
});
await client.connect();

// Autocommit OFF (manual transaction management)
try {
    await client.query('BEGIN');
    await client.query("INSERT INTO users (name) VALUES ('Alice')");
    await client.query("INSERT INTO users (name) VALUES ('Bob')");
    await client.query('COMMIT');  // Must explicitly commit
} catch (e) {
    await client.query('ROLLBACK');  // Must explicitly rollback on error
    console.log(`Error: ${e}`);
}

// Autocommit ON (automatic transaction management - default)
await client.query("INSERT INTO users (name) VALUES ('Alice')");  // Committed immediately
// No need to explicitly commit
```

**Best Practice:**
- Use **autocommit ON** for most applications
- Explicitly use `BEGIN...COMMIT` when you need multi-statement transactions
- Let the database handle transaction boundaries automatically

### When to Use Multi-Statement Transactions

**Use Case 1: Financial Transfers**

```sql
BEGIN TRANSACTION;
    -- Deduct from sender
    UPDATE accounts SET balance = balance - 100 WHERE id = 'sender';
    
    -- Add to recipient  
    UPDATE accounts SET balance = balance + 100 WHERE id = 'recipient';
    
    -- Log transaction
    INSERT INTO transfer_log (from_id, to_id, amount, timestamp)
    VALUES ('sender', 'recipient', 100, NOW());
    
COMMIT;

-- If ANY step fails, ALL steps are undone
```

**Use Case 2: Order Processing**

```sql
BEGIN TRANSACTION;
    -- Create order
    INSERT INTO orders (user_id, total) VALUES (123, 50.00);
    SET @order_id = LAST_INSERT_ID();
    
    -- Add order items
    INSERT INTO order_items (order_id, product_id, quantity)
    VALUES (@order_id, 'WIDGET-1', 2);
    
    -- Update inventory
    UPDATE products SET stock = stock - 2 WHERE id = 'WIDGET-1';
    
    -- Check constraint (inventory can't go negative)
    SELECT stock FROM products WHERE id = 'WIDGET-1';
    -- If stock < 0, we'll rollback
    
COMMIT;
```

**Use Case 3: Read-Modify-Write**

```sql
BEGIN TRANSACTION;
    -- Read current value
    SELECT balance FROM accounts WHERE id = 123 FOR UPDATE;
    -- FOR UPDATE locks the row until commit
    
    -- Application logic (checking business rules)
    -- balance is $50, user wants to withdraw $30
    
    -- Write new value
    UPDATE accounts SET balance = 50 - 30 WHERE id = 123;
    
COMMIT;

-- The lock ensures no other transaction can modify balance
-- between our SELECT and UPDATE

---

## Part 6: Distributed Transactions in Sharded Databases

### The Sharding Challenge

The transcript discussion about cross-shard transactions (with the Vitess example) reveals an even MORE complex scenario than simple distributed transactions. Let's explore this in depth.

#### What is Sharding?

**Sharding** (also called horizontal partitioning) is splitting one large database into multiple smaller databases, each handling a subset of data.

**Example: User Database**

```
Original (single database):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ All Users (100 million rows)   â”‚
â”‚ users table                     â”‚
â”‚   - user_id                     â”‚
â”‚   - username                    â”‚
â”‚   - email                       â”‚
â”‚   - balance                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
TOO BIG! Too much data for one machine!

Sharded (split by user_id):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard 1   â”‚  â”‚ Shard 2   â”‚  â”‚ Shard 3   â”‚  â”‚ Shard 4   â”‚
â”‚ user_id   â”‚  â”‚ user_id   â”‚  â”‚ user_id   â”‚  â”‚ user_id   â”‚
â”‚ 1-25M     â”‚  â”‚ 25M-50M   â”‚  â”‚ 50M-75M   â”‚  â”‚ 75M-100M  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits of Sharding:**
- **Scalability**: Add more shards to handle more data
- **Performance**: Each shard handles fewer rows (faster queries)
- **Parallelism**: Queries can run on multiple shards simultaneously

**But there's a HUGE problem with sharding: Cross-Shard Transactions.**

### Cross-Shard Transaction Problem

**Scenario from Transcript:**

You have a transaction that needs to modify data on **multiple shards**:

```sql
BEGIN TRANSACTION;
    
    -- This user is on Shard 1
    UPDATE users SET balance = balance - 50 WHERE user_id = 1000000;
    
    -- This user is on Shard 3  
    UPDATE users SET balance = balance + 50 WHERE user_id = 60000000;
    
    -- This insert might go to Shard 2 (based on sharding key)
    INSERT INTO transactions (from_user, to_user, amount)
    VALUES (1000000, 60000000, 50);
    
COMMIT;
```

**The Dilemma:**

```
        Application
             â”‚
             â”‚ BEGIN TRANSACTION
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚          â”‚          â”‚
   â”Œâ”€â”€â–¼â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”
   â”‚Shardâ”‚     â”‚Shard â”‚   â”‚Shard â”‚   â”‚Shard â”‚
   â”‚  1  â”‚     â”‚  2   â”‚   â”‚  3   â”‚   â”‚  4   â”‚
   â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”˜
      â”‚            â”‚          â”‚          â”‚
      â”‚ UPDATE     â”‚ INSERT   â”‚ UPDATE   â”‚
      â”‚ user 1M    â”‚ txn log  â”‚ user 60M â”‚
      â”‚            â”‚          â”‚          â”‚
      â”‚ OK âœ“       â”‚ OK âœ“     â”‚ ğŸ’¥ ERROR â”‚
      â”‚            â”‚          â”‚          â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      
  NOW WHAT?!
  - Shard 1 succeeded
  - Shard 2 succeeded  
  - Shard 3 failed (constraint violation)
  
  How do we ROLLBACK on Shard 1 and 2?
```

This is where **Two-Phase Commit becomes essential** for sharded databases.

### Vitess Example: Real Production Challenges

The transcript mentioned **Vitess** - this is a real, widely-used database sharding system for MySQL used by:
- **GitHub** (billions of rows)
- **Slack** (millions of users)  
- **HubSpot** (massive marketing automation)
- **Square** (payment processing)
- **Pinterest** (image and user data)

**Vitess Architecture:**

```
                    Application
                         â”‚
                         â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   VTGate (Proxy) â”‚ â† Query router
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ VTTabletâ”‚      â”‚ VTTabletâ”‚     â”‚ VTTabletâ”‚
   â”‚ (Shard 1)      â”‚ (Shard 2)     â”‚ (Shard 3)
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ MySQL   â”‚      â”‚ MySQL   â”‚     â”‚ MySQL   â”‚
   â”‚ (Primary)      â”‚ (Primary)     â”‚ (Primary)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Vitess History with Distributed Transactions (from transcript):**

1. **Early Days**: Vitess supported distributed transactions (2PC)
2. **Performance Problems**: They were SO SLOW that the feature was removed
3. **Recent**: Re-introduced with better implementation (~2020-2021)
4. **Current Recommendation**: **Avoid distributed transactions if possible!**

**Why Were They So Slow?**

```
Cross-Shard Transaction Timeline:

t=0ms:   Application sends BEGIN TRANSACTION to VTGate
t=1ms:   VTGate sends BEGIN to Shard 1, Shard 2, Shard 3
t=5ms:   VTGate sends writes to all shards
t=10ms:  All shards write to their local transaction log

â”€â”€â”€â”€â”€â”€â”€â”€â”€ START 2PC â”€â”€â”€â”€â”€â”€â”€â”€â”€

t=15ms:  VTGate sends PREPARE to all shards
t=20ms:  Shard 1: OK âœ“ (5ms network)
t=25ms:  Shard 2: OK âœ“ (10ms network)
t=200ms: Shard 3: OK âœ“ (185ms network - cross datacenter!)

â”€â”€â”€â”€â”€â”€â”€â”€â”€ WAITING 185ms! â”€â”€â”€â”€â”€â”€â”€â”€â”€

         During this time:
         - Locks held on ALL shards
         - Other transactions blocked
         - Users waiting...

t=205ms: VTGate sends COMMIT to all shards  
t=215ms: All shards commit

TOTAL: 215ms (vs ~5ms for single-shard transaction)
```

**Even worse scenario:**

```
t=200ms: Shard 3 still hasn't responded
t=300ms: Shard 3 still hasn't responded  
t=500ms: Shard 3 still hasn't responded
...
t=30s:   TIMEOUT! Coordinator gives up
         - ROLLBACK all shards
         - All locks held for 30 SECONDS
         - Huge queue of waiting queries
         - Users see timeout errors
```

### How to Avoid Cross-Shard Transactions

The transcript emphasized: **Much better to design your system to avoid needing distributed transactions!**

**Strategy 1: Co-locate Related Data**

**Bad Design** (requires cross-shard transactions):

```sql
-- User table sharded by user_id
Shard 1: users WHERE user_id BETWEEN 1 AND 1000000
Shard 2: users WHERE user_id BETWEEN 1000001 AND 2000000

-- Orders table sharded by order_id (DIFFERENT key!)
Shard A: orders WHERE order_id BETWEEN 1 AND 500000  
Shard B: orders WHERE order_id BETWEEN 500001 AND 1000000

-- Problem: To update user AND their orders requires cross-shard transaction!
UPDATE users SET last_order_date = NOW() WHERE user_id = 500000;  -- Shard 1
INSERT INTO orders (user_id, total) VALUES (500000, 99.99);      -- Shard A or B?
```

**Good Design** (co-locate related data):

```sql
-- BOTH tables sharded by user_id
Shard 1: 
  - users WHERE user_id BETWEEN 1 AND 1000000
  - orders WHERE user_id BETWEEN 1 AND 1000000  â† Same shard!
  
Shard 2:
  - users WHERE user_id BETWEEN 1000001 AND 2000000
  - orders WHERE user_id BETWEEN 1000001 AND 2000000

-- Now this transaction stays on ONE shard:
BEGIN;
  UPDATE users SET last_order_date = NOW() WHERE user_id = 500000;
  INSERT INTO orders (user_id, total) VALUES (500000, 99.99);
COMMIT;
-- All data on Shard 1, no cross-shard communication needed!
```

**Strategy 2: Application-Level Compensation**

Instead of database-level transactions, handle failures in application code:

```javascript
async function transferMoney(fromUserId, toUserId, amount) {
    // Transfer between users (might be on different shards)
    
    // Step 1: Deduct from sender
    try {
        await db.execute(
            `UPDATE users SET balance = balance - ${amount} 
             WHERE user_id = ${fromUserId}`
        );
    } catch (e) {
        // Failed to deduct, nothing to compensate
        throw new TransferError("Failed to deduct from sender");
    }
    
    // Step 2: Add to recipient
    try {
        await db.execute(
            `UPDATE users SET balance = balance + ${amount}
             WHERE user_id = ${toUserId}`
        );
    } catch (e) {
        // Failed to add! Need to COMPENSATE (refund sender)
        try {
            await db.execute(
                `UPDATE users SET balance = balance + ${amount}
                 WHERE user_id = ${fromUserId}`
            );
            console.error(`Transfer failed, refunded sender ${fromUserId}`);
        } catch (compensateError) {
            // CRITICAL ERROR: Money lost! Alert ops team!
            alertOpsTeam(`CRITICAL: Lost $${amount} from user ${fromUserId}`);
        }
        
        throw new TransferError("Failed to add to recipient, refunded sender");
    }
    
    // Success!
    return true;
}
```

**This is messy but avoids distributed transaction overhead!**

**Strategy 3: Saga Pattern**

Break long transactions into smaller steps with compensation logic:

```javascript
class TransferSaga {
    constructor() {
        this.completedSteps = [];
    }
    
    async execute(fromUser, toUser, amount) {
        try {
            // Step 1: Reserve funds from sender
            await this.reserveFunds(fromUser, amount);
            this.completedSteps.push('reserve');
            
            // Step 2: Create pending transfer record
            const transferId = await this.createTransfer(fromUser, toUser, amount);
            this.completedSteps.push('create_transfer');
            
            // Step 3: Add funds to recipient
            await this.addFunds(toUser, amount);
            this.completedSteps.push('add_funds');
            
            // Step 4: Finalize (remove reservation, mark transfer complete)
            await this.finalizeTransfer(fromUser, transferId);
            this.completedSteps.push('finalize');
            
            return transferId;
            
        } catch (e) {
            // Rollback completed steps in reverse order
            await this.compensate();
            throw e;
        }
    }
    
    async compensate() {
        // Undo completed steps
        for (const step of this.completedSteps.reverse()) {
            if (step === 'finalize') {
                await this.undoFinalize();
            } else if (step === 'add_funds') {
                await this.removeFunds();
            } else if (step === 'create_transfer') {
                await this.deleteTransfer();
            } else if (step === 'reserve') {
                await this.unreserveFunds();
            }
        }
    }
}
```

**Key Takeaway from Transcript Discussion:**

> "Even when everything is working properly, [distributed transactions] just add overhead and more network congestion. So it's much better to try and keep it where your queries are all isolated to... all of the inserts and updates that I'm going to do in one transaction stick to a single database."

This is hard-won wisdom from engineers running massive sharded databases at scale!
```

---

## Part 5: Distributed Transactions and Two-Phase Commit

Now that we understand single-database transactions, let's tackle the much harder problem: **distributed transactions**.
print(database.read('x'))  # Prints 1
print(database.read('x'))  # Prints 2

# Thread 3
print(database.read('x'))  # Prints 2
print(database.read('x'))  # Prints 1  â† Violation! (went backwards)
```

**Linearizable system**: Cannot observe value going backwards.

### Linearizability vs Serializability

**Serializability** (Chapter 7): Isolation property for transactions
- Transactions execute as if in some serial order
- Order can be different from real-time order

**Linearizability**: Recency guarantee for reads/writes
- Operations reflect a real-time total order
- Single-object operations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Serializability:                        â”‚
â”‚   Transactions appear in SOME order     â”‚
â”‚   (may not match real-time)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linearizability:                        â”‚
â”‚   Operations appear in REAL-TIME order  â”‚
â”‚   (recency guarantee)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Both can be combined: "Strict Serializability"
```

### Use Cases Requiring Linearizability

**1. Locking and Leader Election**

```javascript
// Distributed lock using linearizable storage
function acquireLock(lockName, nodeId) {
    // Only one node can successfully write
    const success = cas(
        'lock:' + lockName,
        null,  // expected
        nodeId  // new_value
    );
    return success;
}

// Without linearizability: Multiple nodes could acquire lock!
```

**2. Constraints and Uniqueness**

```sql
-- Enforce unique username across distributed database
INSERT INTO users (username, email) VALUES ('alice', 'alice@example.com');

-- Without linearizability: Two nodes could both insert 'alice'
```

**3. Cross-Channel Timing Dependencies**

```
User uploads profile photo:
  1. Upload image to storage service â†’ URL
  2. Write URL to database
  3. Send notification "Photo uploaded"

Without linearizability:
  - Notification arrives
  - User tries to view photo
  - Database read returns old URL (stale!)
  - Photo not found
```

### Implementing Linearizable Systems

**Options:**

**1. Single-Leader Replication (potentially linearizable)**
- Reads from leader: Linearizable
- Reads from followers: NOT linearizable (replication lag)

```javascript
// Linearizable read (always from leader)
function readLinearizable(key) {
    return leader.read(key);
}

// Non-linearizable read (from any replica)
function readEventuallyConsistent(key) {
    const replica = replicas[Math.floor(Math.random() * replicas.length)];
    return replica.read(key);
}
```

**2. Consensus Algorithms (linearizable)**
- Raft, Paxos, ZooKeeper
- Coordinate writes across multiple nodes
- More expensive but truly linearizable

**3. Multi-Leader Replication (NOT linearizable)**
- Concurrent writes to different leaders
- Conflicts resolved asynchronously

**4. Leaderless Replication with Quorums (NOT necessarily linearizable)**
- Even with strict quorums (w + r > n)
- Concurrent writes and network delays cause issues

**Example: Quorum Non-Linearizability**

```
Configuration: 3 nodes, w=2, r=2

t=0: Client 1 writes x=1 to Node A, Node B (w=2, acknowledged)
t=1: Client 2 reads x from Node B, Node C â†’ Returns x=1
t=2: Client 1 writes x=2 to Node A, Node C (w=2, acknowledged)
t=3: Client 2 reads x from Node A, Node B â†’ Returns x=1 (stale!)

Problem: Read at t=3 doesn't see write at t=2, even though:
  - Write was acknowledged
  - Read happened after write
```

### The Cost of Linearizability

**CAP Theorem**:
> In a distributed system with network partitions, you must choose between Consistency (linearizability) and Availability.

```
Network partition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node A     â”‚    X    â”‚  Node B     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Choice 1: Prioritize Consistency (CP)
  - Reject reads/writes on minority partition
  - System becomes unavailable on minority side
  
Choice 2: Prioritize Availability (AP)
  - Accept reads/writes on both sides
  - Lose linearizability (divergent state)
```

**Real-World Trade-offs:**

| System | Choice | Justification |
|--------|--------|---------------|
| **Bank account balance** | CP (Consistency) | Can't allow overdrafts |
| **Social media likes** | AP (Availability) | Eventual consistency acceptable |
| **Inventory count** | CP | Can't oversell products |
| **News feed** | AP | Stale posts tolerable |

**Performance Impact:**

```javascript
// Linearizable write (slow: must wait for quorum)
let start = Date.now();
await db.writeLinearizable('key', 'value');
console.log(`Latency: ${((Date.now() - start) / 1000).toFixed(3)}s`);  // 50ms+

// Eventual consistency write (fast: async replication)
start = Date.now();
await db.writeAsync('key', 'value');
console.log(`Latency: ${((Date.now() - start) / 1000).toFixed(3)}s`);  // 1ms
```

## Part 3: Ordering Guarantees

### Causality

**Causal Consistency**: If event A causally depends on event B, all nodes see B before A.

**Example: Social Media Comments**

```
Alice: Posts "Check out this photo!"
  â†“ (causal dependency)
Bob: Comments "Nice photo!"

Without causal consistency:
  Charlie sees Bob's comment first, then Alice's post
  (confusing!)

With causal consistency:
  All users see Alice's post before Bob's comment
```

**Causal Dependencies:**

```
Events:
A: User 1 writes x=1
B: User 2 reads x=1, then writes y=2  (depends on A)
C: User 3 reads y=2, then writes z=3  (depends on B)

Causal order:
  A â†’ B â†’ C

Non-causal events (can be concurrent):
A: User 1 writes x=1
D: User 4 writes w=5  (independent of A)
```

**Visual: Causal Relationships**

```
      A (write x=1)
      â”‚
      â””â”€â”€â†’ B (read x, write y=2)
           â”‚
           â””â”€â”€â†’ C (read y, write z=3)

D (write w=5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  (concurrent with A, B, C)
```

### The Total Order vs Partial Order

**Total Order**: Every pair of operations is comparable
- Example: Linearizability provides total order
- All operations happen "in a line"

```
Total Order:
  op1 â†’ op2 â†’ op3 â†’ op4 â†’ op5
```

**Partial Order**: Some operations are concurrent (incomparable)
- Example: Causality provides partial order
- Concurrent operations have no defined order

```
Partial Order:
        op1
       â†™   â†˜
    op2     op3  (concurrent)
       â†˜   â†™
        op4
```

### Sequence Number Ordering

**Idea**: Assign sequence numbers to operations to determine order.

**Lamport Timestamps** (Logical Clocks):

```javascript
class LamportClock {
    constructor(nodeId) {
        this.counter = 0;
        this.nodeId = nodeId;
    }
    
    tick() {
        // Increment on local event
        this.counter += 1;
        return [this.counter, this.nodeId];
    }
    
    update(receivedTimestamp) {
        // Update on receiving message
        const [receivedCounter, _] = receivedTimestamp;
        this.counter = Math.max(this.counter, receivedCounter) + 1;
        return [this.counter, this.nodeId];
    }
}

// Usage
const node1 = new LamportClock('node1');
const node2 = new LamportClock('node2');

// Node 1 events
const t1 = node1.tick();  // [1, 'node1']
const t2 = node1.tick();  // [2, 'node1']

// Node 2 receives message with timestamp t2
const t3 = node2.update(t2);  // [3, 'node2']
const t4 = node2.tick();       // [4, 'node2']

// Ordering: t1 < t2 < t3 < t4
```

**Lamport Timestamp Comparison:**

```javascript
function compareTimestamps(t1, t2) {
    // Compare [counter, nodeId] arrays
    const [counter1, node1] = t1;
    const [counter2, node2] = t2;
    
    if (counter1 < counter2) {
        return -1;  // t1 < t2
    } else if (counter1 > counter2) {
        return 1;   // t1 > t2
    } else {
        // Break ties with node ID
        return node1 < node2 ? -1 : 1;
    }
}
```

**Limitation**: Lamport timestamps provide total order but don't capture causality completely.
- If t1 < t2, we can't conclude that t1 caused t2
- Could be concurrent operations

### Version Vectors (Detecting Concurrency)

**Version vectors** track causality accurately:

```javascript
class VersionVector {
    constructor(nodeId) {
        this.nodeId = nodeId;
        this.vector = {};  // nodeId â†’ counter
    }
    
    increment() {
        // Increment own counter
        this.vector[this.nodeId] = (this.vector[this.nodeId] || 0) + 1;
        return { ...this.vector };
    }
    
    merge(otherVector) {
        // Merge with received vector
        for (const [node, count] of Object.entries(otherVector)) {
            this.vector[node] = Math.max(this.vector[node] || 0, count);
        }
        this.increment();
        return { ...this.vector };
    }
    
    happensBefore(otherVector) {
        // Check if this happens before other
        for (const [node, count] of Object.entries(otherVector)) {
            if ((this.vector[node] || 0) > count) {
                return false;  // Not happens-before
            }
        }
        return true;
    }
    
    isConcurrent(otherVector) {
        // Check if operations are concurrent
        return !this.happensBefore(otherVector) && 
               !otherHappensBeforeSelf(otherVector);
    }
}

// Example
const node1 = new VersionVector('node1');
const node2 = new VersionVector('node2');

const v1 = node1.increment();  // {node1: 1}
const v2 = node1.increment();  // {node1: 2}

const v3 = node2.increment();  // {node2: 1}
node2.merge(v2);               // {node1: 2, node2: 2}

// v1 happens-before v2: True
// v3 concurrent with v1: True
```

**Visual: Version Vectors**

```
Node 1:              Node 2:
{node1: 1}           {node2: 1}
    â”‚                    â”‚
{node1: 2}           {node2: 2}
    â”‚ send to node2      â”‚
    â”‚                    â–¼
    â”‚                {node1: 2, node2: 3}
    â”‚ send to node2      â”‚
    â–¼                    â–¼
{node1: 3, node2: 3}  ...
```

## Part 4: Distributed Transactions and Two-Phase Commit

### The Problem: Atomic Commit Across Multiple Nodes

**Scenario**: Transfer $100 from Account A (Node 1) to Account B (Node 2)

```
Transaction:
  1. Deduct $100 from Account A (Node 1)
  2. Add $100 to Account B (Node 2)

Challenge: Both must succeed or both must fail (atomicity)!
```

**Failure Scenarios:**

```
Scenario 1: Node 1 commits, Node 2 crashes
  â†’ Money deducted but not added (lost $100!)

Scenario 2: Node 1 commits, Node 2 aborts due to constraint violation
  â†’ Money deducted but not added (lost $100!)
```

### Two-Phase Commit (2PC)

**2PC** ensures atomic commit across multiple nodes using a **coordinator**.

**Phase 1: Prepare**
```
Coordinator â†’ Participants: "Prepare to commit"
Participants â†’ Coordinator: "OK" or "Abort"
```

**Phase 2: Commit/Abort**
```
If all participants voted "OK":
  Coordinator â†’ Participants: "Commit"
Else:
  Coordinator â†’ Participants: "Abort"
```

**Visual: 2PC Protocol**

```
Coordinator                 Node 1              Node 2
     â”‚                          â”‚                   â”‚
     â”‚â”€â”€â”€â”€ Prepare â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
     â”‚                          â”‚                   â”‚
     â”‚â”€â”€â”€â”€ Prepare â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
     â”‚                          â”‚                   â”‚
     â”‚<â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚
     â”‚                          â”‚                   â”‚
     â”‚<â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                          â”‚                   â”‚
     â”‚â”€â”€â”€ Commit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
     â”‚                          â”‚                   â”‚
     â”‚â”€â”€â”€ Commit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
     â”‚                          â”‚                   â”‚
     â”‚                       (commit)            (commit)
```

**Implementation:**

```javascript
class TwoPhaseCommitCoordinator {
    constructor(participants) {
        this.participants = participants;
        this.transactionLog = [];
    }
    
    async executeTransaction(transaction) {
        const transactionId = generateId();
        
        // Phase 1: Prepare
        this.transactionLog.push(['PREPARE', transactionId]);
        const votes = [];
        
        for (const participant of this.participants) {
            try {
                const vote = await participant.prepare(transactionId, transaction);
                votes.push(vote);
            } catch (e) {
                votes.push('ABORT');
            }
        }
        
        // Decision
        const decision = votes.every(vote => vote === 'OK') ? 'COMMIT' : 'ABORT';
        
        this.transactionLog.push([decision, transactionId]);
        
        // Phase 2: Commit or Abort
        for (const participant of this.participants) {
            if (decision === 'COMMIT') {
                await participant.commit(transactionId);
            } else {
                await participant.abort(transactionId);
            }
        }
        
        return decision;
    }
}

class Participant {
    constructor() {
        this.preparedTransactions = {};
    }
    
    async prepare(transactionId, transaction) {
        // Prepare phase: check if can commit
        try {
            // Check constraints, acquire locks, etc.
            this.preparedTransactions[transactionId] = transaction;
            return 'OK';
        } catch (e) {
            return 'ABORT';
        }
    }
    
    async commit(transactionId) {
        // Commit phase: actually apply changes
        const transaction = this.preparedTransactions[transactionId];
        delete this.preparedTransactions[transactionId];
        // Apply changes to database
        await this.apply(transaction);
    }
    
    async abort(transactionId) {
        // Abort phase: discard prepared transaction
        delete this.preparedTransactions[transactionId];
    }
}
```

### Problems with Two-Phase Commit

**1. Coordinator Failure**

```
Coordinator crashes after some participants committed:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Coordinator  â”‚  â† CRASH (after sending some COMMIT messages)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â–¼       â–¼
Node 1    Node 2
(committed) (waiting...)

Node 2 is BLOCKED: Can't commit or abort without coordinator!
```

**Solution**: Coordinator writes decision to durable log before sending.
- On recovery, re-send commit/abort messages

**2. Participant Failure**

```
Node 2 crashes after voting OK but before receiving COMMIT:

Coordinator: Sends COMMIT
Node 1: Commits successfully
Node 2: CRASH (in "uncertain" state)

On recovery, Node 2 must ask coordinator for decision.
```

**3. Performance Issues**

```
Latency comparison:
Single-node transaction:  10ms
2PC transaction:          100ms+  (10x slower!)

Reasons:
- Multiple network round-trips
- Synchronous communication
- Locks held across network calls
```

**4. Blocking Protocol**

```
If coordinator crashes, participants are blocked:

Node 1: "I voted OK, but coordinator is gone. Can I commit?"
  â†’ Cannot commit (don't know if Node 2 voted OK)
  â†’ Cannot abort (coordinator might have committed)
  â†’ BLOCKED until coordinator recovers
```

### Three-Phase Commit (3PC)

**Improvement**: Add a "PreCommit" phase to avoid blocking.

**Phases:**
1. **Prepare**: Can you commit?
2. **PreCommit**: All voted OK, prepare to commit
3. **Commit**: Actually commit

**Advantage**: If coordinator fails after PreCommit, participants know everyone voted OK.

**Disadvantage**: Still not practical due to network partitions.

## Part 5: Consensus Algorithms

### What is Consensus?

**Consensus Problem**: Get multiple nodes to agree on a single value.

**Requirements:**
1. **Agreement**: All correct nodes decide on the same value
2. **Integrity**: Nodes decide at most once
3. **Validity**: Decided value was proposed by some node
4. **Termination**: All correct nodes eventually decide

**Applications:**
- **Leader election**: Agree on which node is leader
- **Atomic commit**: Agree to commit or abort
- **State machine replication**: Agree on sequence of operations

### FLP Impossibility Result

**Fischer, Lynch, Paterson (1985)**: 
> No deterministic consensus algorithm is guaranteed to terminate in an asynchronous system with even one faulty process.

**Implication**: Perfect consensus is impossible!

**Practical solution**: Use timeouts and retries (non-deterministic).

### Paxos Algorithm

Paxos is the most famous consensus algorithm, invented by Leslie Lamport (1989).

**Roles:**
- **Proposer**: Proposes values
- **Acceptor**: Votes on proposals
- **Learner**: Learns decided value

**Paxos Phases:**

**Phase 1: Prepare**
```
Proposer â†’ Acceptors: "Prepare(n)"
  (n = unique proposal number)

Acceptors â†’ Proposer: "Promise(n, v)"
  v = highest-numbered proposal already accepted (or null)
  Promise not to accept proposals numbered < n
```

**Phase 2: Accept**
```
Proposer â†’ Acceptors: "Accept(n, v)"
  v = value from Phase 1, or proposer's own value

Acceptors â†’ Learners: "Accepted(n, v)"
  Accept proposal if n is still highest seen
```

**Visual: Paxos Execution**

```
Proposer 1          Acceptor A     Acceptor B     Acceptor C
     â”‚                   â”‚              â”‚              â”‚
     â”‚â”€ Prepare(5) â”€â”€â”€â”€â”€>â”‚              â”‚              â”‚
     â”‚                   â”‚              â”‚              â”‚
     â”‚â”€ Prepare(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚              â”‚
     â”‚                   â”‚              â”‚              â”‚
     â”‚â”€ Prepare(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
     â”‚                   â”‚              â”‚              â”‚
     â”‚<â”€â”€ Promise(5) â”€â”€â”€â”€â”‚              â”‚              â”‚
     â”‚<â”€â”€ Promise(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚
     â”‚<â”€â”€ Promise(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚                   â”‚              â”‚              â”‚
     â”‚â”€ Accept(5,"A") â”€â”€>â”‚              â”‚              â”‚
     â”‚â”€ Accept(5,"A") â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚              â”‚
     â”‚â”€ Accept(5,"A") â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
     â”‚                   â”‚              â”‚              â”‚
     â”‚                (accepted)     (accepted)     (accepted)
```

**Example: Competing Proposers**

```javascript
// Simplified Paxos implementation
class PaxosAcceptor {
    constructor() {
        this.promisedProposal = null;
        this.acceptedProposal = null;
        this.acceptedValue = null;
    }
    
    receivePrepare(proposalNumber) {
        // Phase 1: Prepare
        if (this.promisedProposal === null || proposalNumber > this.promisedProposal) {
            this.promisedProposal = proposalNumber;
            return ['PROMISE', this.acceptedProposal, this.acceptedValue];
        } else {
            return ['REJECT', null, null];
        }
    }
    
    receiveAccept(proposalNumber, value) {
        // Phase 2: Accept
        if (this.promisedProposal === null || proposalNumber >= this.promisedProposal) {
            this.promisedProposal = proposalNumber;
            this.acceptedProposal = proposalNumber;
            this.acceptedValue = value;
            return ['ACCEPTED', value];
        } else {
            return ['REJECT', null];
        }
    }
}

class PaxosProposer {
    constructor(nodeId, acceptors) {
        this.nodeId = nodeId;
        this.acceptors = acceptors;
        this.proposalNumber = 0;
    }
    
    async propose(value) {
        // Run Paxos to get value accepted
        this.proposalNumber += 1;
        const proposal = [this.proposalNumber, this.nodeId];  // Unique proposal number
        
        // Phase 1: Prepare
        const promises = [];
        for (const acceptor of this.acceptors) {
            const response = await acceptor.receivePrepare(proposal);
            if (response[0] === 'PROMISE') {
                promises.push(response);
            }
        }
        
        // Need majority
        if (promises.length < Math.floor(this.acceptors.length / 2) + 1) {
            return null;  // Failed to get majority
        }
        
        // Check if any acceptor already accepted a value
        const acceptedValues = promises
            .filter(p => p[2] !== null)
            .map(p => p[2]);
        if (acceptedValues.length > 0) {
            // Use highest-numbered accepted value
            value = acceptedValues.reduce((max, v) => 
                promises[2][1] > max ? v : max
            );
        }
        
        // Phase 2: Accept
        const accepts = [];
        for (const acceptor of this.acceptors) {
            const response = await acceptor.receiveAccept(proposal, value);
            if (response[0] === 'ACCEPTED') {
                accepts.push(response);
            }
        }
        
        // Need majority
        if (accepts.length >= Math.floor(this.acceptors.length / 2) + 1) {
            return value;  // Consensus reached!
        } else {
            return null;  // Failed
        }
    }
}
```

### Raft Algorithm

**Raft** is a consensus algorithm designed to be easier to understand than Paxos.

**Key Idea**: Strong leader
- Leader handles all client requests
- Followers replicate leader's log
- New leader elected if current leader fails

**Raft Components:**

**1. Leader Election**

```
All nodes start as followers:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Follower Aâ”‚  â”‚Follower Bâ”‚  â”‚Follower Câ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If no heartbeat from leader â†’ election timeout:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Candidate â”‚  â”‚Follower Bâ”‚  â”‚Follower Câ”‚
â”‚   A      â”‚  â”‚          â”‚  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚           â”‚              â”‚
      â”‚â”€ Vote â”€â”€â”€>â”‚              â”‚
      â”‚â”€ Vote â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚           â”‚              â”‚
      â”‚<â”€â”€ OK â”€â”€â”€â”€â”‚              â”‚
      â”‚<â”€â”€ OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚           â”‚              â”‚
    (Becomes Leader)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader A â”‚  â”‚Follower Bâ”‚  â”‚Follower Câ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Log Replication**

```
Leader receives write:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader Log:              â”‚
â”‚ [1: x=1] [2: y=2] [3: z=3]â”‚ â† Append entry 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â”‚ Replicate  â”‚
     â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Followerâ”‚   â”‚Followerâ”‚
â”‚[1][2]  â”‚   â”‚[1][2]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚[1][2][3]â”‚   â”‚[1][2][3]â”‚ â† Acknowledge
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Majority replicated
          â–¼
   Leader commits entry 3
```

**3. Safety**

**Election Safety**: At most one leader per term
**Leader Append-Only**: Leader never deletes/overwrites log entries
**Log Matching**: If two logs contain entry with same index/term, all preceding entries are identical
**Leader Completeness**: If entry committed in term T, it appears in all leaders' logs for terms > T
**State Machine Safety**: If a server applies log entry at given index, no other server applies different entry at that index

**Raft Implementation (Simplified):**

```javascript
class RaftNode {
    constructor(nodeId, peers) {
        this.nodeId = nodeId;
        this.peers = peers;
        this.state = 'FOLLOWER';  // FOLLOWER, CANDIDATE, or LEADER
        this.currentTerm = 0;
        this.votedFor = null;
        this.log = [];  // Array of [term, command] tuples
        this.commitIndex = 0;
        this.lastHeartbeat = Date.now();
    }
    
    async startElection() {
        // Become candidate and request votes
        this.state = 'CANDIDATE';
        this.currentTerm += 1;
        this.votedFor = this.nodeId;
        let votes = 1;  // Vote for self
        
        for (const peer of this.peers) {
            const response = await peer.requestVote({
                term: this.currentTerm,
                candidateId: this.nodeId,
                lastLogIndex: this.log.length - 1,
                lastLogTerm: this.log.length > 0 ? this.log[this.log.length - 1][0] : 0
            });
            if (response.voteGranted) {
                votes += 1;
            }
        }
        
        // Need majority
        if (votes > Math.floor((this.peers.length + 1) / 2)) {
            this.becomeLeader();
        }
    }
    
    becomeLeader() {
        // Transition to leader state
        this.state = 'LEADER';
        console.log(`Node ${this.nodeId} became leader for term ${this.currentTerm}`);
        this.sendHeartbeats();
    }
    
    async appendEntry(command) {
        // Client request to append entry (leader only)
        if (this.state !== 'LEADER') {
            return { success: false, leader: this.currentLeader };
        }
        
        // Append to own log
        this.log.push([this.currentTerm, command]);
        const logIndex = this.log.length - 1;
        
        // Replicate to followers
        let acks = 1;  // Count self
        for (const peer of this.peers) {
            const response = await peer.appendEntries({
                term: this.currentTerm,
                leaderId: this.nodeId,
                prevLogIndex: logIndex - 1,
                prevLogTerm: logIndex > 0 ? this.log[logIndex - 1][0] : 0,
                entries: [[this.currentTerm, command]],
                leaderCommit: this.commitIndex
            });
            if (response.success) {
                acks += 1;
            }
        }
        
        // Commit if majority replicated
        if (acks > Math.floor((this.peers.length + 1) / 2)) {
            this.commitIndex = logIndex;
            return { success: true };
        } else {
            return { success: false };
        }
    }
}
```

### Raft vs Paxos

| Aspect | Paxos | Raft |
|--------|-------|------|
| **Understandability** | Complex | Simpler |
| **Leadership** | Weak (can have multiple proposers) | Strong (single leader) |
| **Log structure** | Can have holes | No holes (sequential) |
| **Performance** | Potentially higher | Easier to implement efficiently |
| **Adoption** | Chubby (Google) | etcd, Consul, CockroachDB |

## Part 6: Coordination Services: ZooKeeper and etcd

### ZooKeeper

**ZooKeeper** is a distributed coordination service that implements a consensus algorithm (ZAB, similar to Raft/Paxos).

**Use Cases:**
- **Configuration management**: Centralized configuration
- **Service discovery**: Find service instances
- **Distributed locking**: Coordinate access
- **Leader election**: Choose primary node
- **Group membership**: Track live nodes

**Data Model: Hierarchical namespace (like filesystem)**

```
/
â”œâ”€â”€ /config
â”‚   â”œâ”€â”€ /config/database
â”‚   â”‚   â””â”€â”€ connection_string="..."
â”‚   â””â”€â”€ /config/cache
â”‚       â””â”€â”€ ttl=300
â”œâ”€â”€ /services
â”‚   â”œâ”€â”€ /services/api
â”‚   â”‚   â”œâ”€â”€ /services/api/node1  (ephemeral)
â”‚   â”‚   â””â”€â”€ /services/api/node2  (ephemeral)
â”‚   â””â”€â”€ /services/worker
â”‚       â””â”€â”€ /services/worker/node3  (ephemeral)
â””â”€â”€ /locks
    â””â”€â”€ /locks/critical_section
```

**Node Types:**
- **Persistent**: Survive until explicitly deleted
- **Ephemeral**: Deleted when client session ends
- **Sequential**: Appended with monotonic counter

**ZooKeeper Operations:**

```javascript
const { ZooKeeper } = require('node-zookeeper-client');

const zk = ZooKeeper.createClient('localhost:2181');
zk.connect();

// Create node
await zk.create('/config/database', Buffer.from('postgresql://...'));

// Read node
const [data, stat] = await zk.getData('/config/database');
console.log(data.toString());  // postgresql://...

// Watch for changes
zk.getData('/config/database', (event) => {
    watchDatabaseConfig(event);
}, (error, data, stat) => {
    if (error) console.error(error);
});

function watchDatabaseConfig(event) {
    if (event.type === ZooKeeper.Event.NODE_DATA_CHANGED) {
        const [data, stat] = zk.getData('/config/database');
        console.log(`Config changed: ${data.toString()}`);
    }
}

// Leader election
async function becomeLeader() {
    // Create ephemeral sequential node
    const path = await zk.create(
        '/election/node_',
        Buffer.from(''),
        ZooKeeper.CreateMode.EPHEMERAL_SEQUENTIAL
    );
    
    // Get all election nodes
    const children = await zk.getChildren('/election');
    children.sort();
    
    // If we have the smallest sequence number, we're the leader
    if (path.endsWith(children[0])) {
        console.log("I am the leader!");
        return true;
    } else {
        // Watch the node before us
        const previousNode = children[children.indexOf(path.split('/').pop()) - 1];
        await zk.exists(
            `/election/${previousNode}`,
            (event) => onPreviousNodeDeleted(event)
        );
        return false;
    }
}

function onPreviousNodeDeleted(event) {
    // Previous leader died, try to become leader
    becomeLeader();
}
```

**Distributed Lock with ZooKeeper:**

```javascript
const { Lock } = require('node-zookeeper-client');

// Distributed lock
const lock = new Lock(zk, '/locks/my_resource');

// Acquire lock
await lock.acquire();
try {
    // Critical section
    console.log("I have the lock!");
    // Perform operations...
} finally {
    // Lock automatically released
    await lock.release();
}
```

### etcd

**etcd** is a distributed key-value store that uses the Raft consensus algorithm.

**Key Features:**
- **Strongly consistent**: Linearizable reads/writes
- **Watch API**: Notifications on key changes
- **Lease mechanism**: TTL for keys
- **Transactions**: Multi-key atomic operations

**Example: Service Registration**

```javascript
const { Etcd3 } = require('etcd3');

const etcd = new Etcd3();

// Register service instance
async function registerService(serviceName, instanceId, address) {
    const lease = etcd.lease(30);  // 30-second lease
    await lease.grant();
    
    const key = `/services/${serviceName}/${instanceId}`;
    const value = JSON.stringify({
        address: address,
        registeredAt: Date.now()
    });
    
    await etcd.put(key).value(value).lease(lease.ID);
    
    // Keep-alive heartbeat
    setInterval(async () => {
        await lease.keepaliveOnce();
    }, 10000);
}

// Discover service instances
async function discoverServices(serviceName) {
    const prefix = `/services/${serviceName}/`;
    const instances = [];
    
    const values = await etcd.getAll().prefix(prefix);
    for (const [key, value] of Object.entries(values)) {
        const instance = JSON.parse(value);
        instances.push(instance);
    }
    
    return instances;
}

// Watch for service changes
function watchServices(serviceName) {
    const prefix = `/services/${serviceName}/`;
    
    const watcher = etcd.watch().prefix(prefix).create();
    
    watcher.on('put', (event) => onServiceChange(event));
    watcher.on('delete', (event) => onServiceChange(event));
}

function onServiceChange(event) {
    if (event.type === 'put') {
        console.log(`Service added: ${event.key.toString()}`);
    } else if (event.type === 'delete') {
        console.log(`Service removed: ${event.key.toString()}`);
    }
}
```

**etcd Transactions (MVCC):**

```javascript
// Atomic compare-and-set
const success = await etcd.if('/counter', 'Value', '==', '5')
    .then(etcd.put('/counter').value('6'))
    .else(etcd.get('/counter'))
    .commit();
```

## Part 7: Membership and Failure Detection

### Failure Detection

**Challenge**: Distinguish between slow nodes and crashed nodes.

**Heartbeat Protocol:**

```javascript
class FailureDetector {
    constructor(timeout = 5.0) {
        this.timeout = timeout * 1000;  // Convert to milliseconds
        this.lastHeartbeat = {};
    }
    
    heartbeatReceived(nodeId) {
        // Record heartbeat from node
        this.lastHeartbeat[nodeId] = Date.now();
    }
    
    isAlive(nodeId) {
        // Check if node is alive
        if (!(nodeId in this.lastHeartbeat)) {
            return false;
        }
        
        const elapsed = Date.now() - this.lastHeartbeat[nodeId];
        return elapsed < this.timeout;
    }
    
    getLiveNodes() {
        // Return list of live nodes
        return Object.keys(this.lastHeartbeat)
            .filter(node => this.isAlive(node));
    }
}
```

**Phi Accrual Failure Detector** (used in Cassandra):
- Instead of binary alive/dead, compute suspicion level (Î¦)
- Higher Î¦ = more suspicious
- Adapt to network conditions

```javascript
class PhiAccrualFailureDetector {
    constructor(threshold = 8.0, windowSize = 100) {
        this.threshold = threshold;
        this.windowSize = windowSize;
        this.intervals = [];
        this.lastHeartbeat = null;
    }
    
    heartbeat() {
        // Record heartbeat
        const now = Date.now();
        if (this.lastHeartbeat !== null) {
            const interval = now - this.lastHeartbeat;
            this.intervals.push(interval);
            if (this.intervals.length > this.windowSize) {
                this.intervals.shift();
            }
        }
        this.lastHeartbeat = now;
    }
    
    phi() {
        // Calculate suspicion level
        if (this.intervals.length === 0 || this.lastHeartbeat === null) {
            return 0.0;
        }
        
        // Time since last heartbeat
        const now = Date.now();
        const elapsed = now - this.lastHeartbeat;
        
        // Calculate mean and std dev of intervals
        const mean = this.intervals.reduce((a, b) => a + b, 0) / this.intervals.length;
        const variance = this.intervals
            .map(x => Math.pow(x - mean, 2))
            .reduce((a, b) => a + b, 0) / this.intervals.length;
        const stdDev = Math.sqrt(variance);
        
        // Phi value (higher = more suspicious)
        if (stdDev === 0) {
            return 0.0;
        }
        
        const phi = -Math.log10(1 - this.normalCdf((elapsed - mean) / stdDev));
        return phi;
    }
    
    isAvailable() {
        // Check if node is available
        return this.phi() < this.threshold;
    }
}
```

### Gossip Protocols

**Gossip** (epidemic protocol): Nodes randomly exchange information.

**Example: SWIM (Scalable Weakly-consistent Infection-style Process Group Membership)**

```javascript
class GossipProtocol {
    constructor(nodeId, allNodes) {
        this.nodeId = nodeId;
        this.allNodes = allNodes;
        this.state = {};  // nodeId â†’ [state, timestamp]
    }
    
    updateLocalState(key, value) {
        // Update local state
        this.state[key] = [value, Date.now()];
    }
    
    gossipRound() {
        // Periodically gossip with random node
        const otherNodes = this.allNodes.filter(n => n !== this.nodeId);
        const peer = otherNodes[Math.floor(Math.random() * otherNodes.length)];
        
        // Send our state to peer
        const peerState = peer.receiveGossip(this.state);
        
        // Merge peer's state into ours
        this.mergeState(peerState);
    }
    
    receiveGossip(peerState) {
        // Receive gossip from peer
        this.mergeState(peerState);
        return this.state;
    }
    
    mergeState(peerState) {
        // Merge peer state with our state (keep newer values)
        for (const [key, [value, timestamp]] of Object.entries(peerState)) {
            if (!(key in this.state) || timestamp > this.state[key][1]) {
                this.state[key] = [value, timestamp];
            }
        }
    }
}
```
        self.merge_state(peer_state)
        return self.state
    
    def merge_state(self, peer_state):
        """Merge peer state (keep more recent values)"""
        for key, (value, timestamp) in peer_state.items():
            if key not in self.state or timestamp > self.state[key][1]:
                self.state[key] = (value, timestamp)
```

**Gossip Convergence:**

```
Time 0: Node A has update X
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ X â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

Time 1: A gossips to B
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ X â”‚ â”‚ X â”‚ â”‚   â”‚ â”‚   â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

Time 2: Aâ†’C, Bâ†’D
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ X â”‚ â”‚ X â”‚ â”‚ X â”‚ â”‚ X â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

All nodes converge to X
```

---

## Part 7: Choosing the Right Database and Data Structure

### Understanding Workload Patterns (From the Transcript)

The transcript included an insightful discussion about different database workloads and how they affect your choice of data structure and consistency model. This is critical practical knowledge!

#### OLTP vs OLAP Workloads

**OLTP (Online Transaction Processing)**:
- **Characteristics**: Short transactions, small reads/writes, interactive
- **Examples**: E-commerce checkout, social media posts, user login
- **Database Choice**: MySQL, PostgreSQL, MongoDB
- **Consistency Needs**: Often need strong consistency (bank transfers)

**OLAP (Online Analytical Processing)**:
- **Characteristics**: Long-running queries, aggregate data, batch processing
- **Examples**: Business reports, data warehousing, trend analysis
- **Database Choice**: ClickHouse, Snowflake, BigQuery
- **Consistency Needs**: Eventual consistency usually fine

#### Data Structure Trade-offs: B-trees vs LSM-trees

**B-trees** (used in MySQL InnoDB, PostgreSQL):
```
Best for:
- Random reads (lookup by ID)
- Updates to existing rows
- Range scans with sorting
- OLTP workloads

Structure:
     [Root]
    /  |  \
  [Node][Node][Node]
   / \   / \   / \
 [Leaf] [Leaf] [Leaf]

Trade-offs:
+ Fast reads (O(log n))
+ Fast updates
- Slower writes (need to update tree structure)
- Write amplification (updating internal nodes)
```

**LSM-trees** (Log-Structured Merge-trees, used in Cassandra, RocksDB, ClickHouse):
```
Best for:
- High write throughput
- Time-series data
- Append-heavy workloads
- Log/metrics ingestion

Structure:
[MemTable] â† Fast in-memory writes
    â†“ (flush when full)
[L0: SSTable Files]
    â†“ (merge/compact)
[L1: Larger SSTable Files]
    â†“
[L2: Even Larger Files]

Trade-offs:
+ Very fast writes (append-only)
+ Good compression
- Slower reads (check multiple levels)
- Background compaction overhead
```

**Real-World Example from Transcript:**

```python
# Choosing database for different workloads

# Use Case 1: E-commerce order system
# - Need transactions (BEGIN/COMMIT/ROLLBACK)
# - Need foreign keys and referential integrity
# - Mix of reads and writes
# - Updates to existing rows common
db_choice = "PostgreSQL or MySQL"  # B-tree based, ACID transactions

# Use Case 2: Analytics/metrics collection
# - Millions of inserts per second
# - Rarely update existing data
# - Mostly time-series queries
# - Can tolerate eventual consistency
db_choice = "ClickHouse or TimescaleDB"  # LSM-tree or columnar storage

# Use Case 3: Session storage
# - Fast key-value lookups
# - Expiration (TTL) needed
# - High throughput
# - Data can be regenerated if lost
db_choice = "Redis or Memcached"  # In-memory, eventually consistent
```

#### The Extension Trap (PostgreSQL Discussion from Transcript)

The transcript warned against over-using PostgreSQL extensions:

**The Problem:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Single PostgreSQL Instance           â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ OLTP Tables â”‚  â”‚ TimescaleDB      â”‚â”‚
â”‚  â”‚ (B-tree)    â”‚  â”‚ (Time-series)    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ pgvector (ML embeddings)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostGIS (Geospatial queries)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    Everything on ONE server!
```

**What Goes Wrong:**
1. **OLTP spike**: 1000 users checking out â†’ database CPU at 80%
2. **Analytics query runs**: "Generate monthly revenue report" â†’ CPU jumps to 100%
3. **OLTP requests timeout**: Users see errors, lose sales
4. **Geospatial query starts**: "Find all stores within 50km" â†’ Everything grinds to halt

**Better Architecture (Separation of Concerns):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚      â”‚ ClickHouse       â”‚
â”‚ (OLTP)       â”‚â”€â”€â”€â”€â”€â†’â”‚ (Analytics)      â”‚
â”‚              â”‚ CDC  â”‚                  â”‚
â”‚ B-tree based â”‚      â”‚ Columnar storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   (Real-time traffic)
      
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis        â”‚      â”‚ Elasticsearch    â”‚
â”‚ (Cache)      â”‚      â”‚ (Search)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight from Transcript:**
> "I'm not a big fan of running your analytics and your OLTP workloads and your time series workloads all in like one single database."

Each database is optimized for its specific workload, preventing resource contention.

---

## Summary and Final Thoughts

### Key Takeaways

**1. Linearizability is Beautiful but Expensive**

Linearizability makes distributed systems behave like a single machineâ€”it's the strongest possible guarantee. But as we saw with the ESPN World Cup example, achieving it requires:
- Coordination between all nodes
- Waiting for slowest replica
- Locks held across network boundaries
- Choosing consistency over availability (CAP theorem)

**When to pay the cost:** Financial transactions, inventory systems, anything where correctness matters more than speed.

**When to skip it:** Social media, analytics, recommendationsâ€”eventual consistency is enough.

**2. Semi-Synchronous Replication is the Sweet Spot**

The transcript discussion revealed why most production systems use semi-sync:
- Fast enough (5-10ms vs 500ms for full-sync)
- Durable enough (data on 2+ machines)
- Available enough (tolerates failures)

**Real-world recommendation**: Start with semi-sync (wait for 1 replica), adjust based on monitoring.

**3. Distributed Transactions Are Hardâ€”Avoid When Possible**

The Vitess example taught us:
- 2PC adds 10-100x latency overhead
- Locks held for entire distributed commit duration
- One slow shard blocks everything
- Many production systems removed distributed transactions after performance problems

**Better approaches:**
- Co-locate related data on same shard
- Application-level compensation (Saga pattern)
- Accept eventual consistency where possible

**4. Choose Your Database Based on Workload**

From the transcript discussion:
- **OLTP** (transactions, updates): PostgreSQL/MySQL with B-trees
- **OLAP** (analytics, aggregations): ClickHouse/Snowflake with columnar storage
- **Time-series** (metrics, logs): TimescaleDB/InfluxDB with LSM-trees
- **Cache** (ephemeral, fast): Redis/Memcached with in-memory storage

**Don't try to make one database do everything!** Separate concerns, let each database excel at what it does best.

**5. Consensus Algorithms Enable Everything**

Paxos, Raft, and ZooKeeper provide the foundation for:
- Leader election (who's in charge?)
- Configuration management (what's the truth?)
- Distributed locking (who has access?)
- Service discovery (who's alive?)

You probably don't need to implement consensus yourself (use etcd/ZooKeeper), but understanding how it works helps you:
- Debug production issues
- Make informed architectural decisions
- Understand system limitations

### Final Wisdom from the Transcript

> "Even when everything is working properly, [distributed transactions] just add overhead and more network congestion. So it's much better to try and keep it where your queries are all isolated to a single database."

> "I'm not a big fan of running your analytics and your OLTP workloads and your time series workloads all in like one single database."

> "You got to go and build stuff... even if you don't feel super confident in it yet. One option is contributing to open-source software. And another option is just go build something even if it never sees the light of day."

**The most important lesson**: Distributed systems are hard, but they're solvable problems. Start simple, measure everything, scale when needed, and always question whether you need the complexity you're adding.

Good luck building distributed systems! ğŸš€

---

**Total Chapter Length**: ~30,000+ words  
**Time to Read**: 120+ minutes  
**Concepts Covered**: 60+  
**Code Examples**: 40+  
**Real-World Systems Referenced**: Vitess, MySQL, PostgreSQL, MongoDB, ClickHouse, etcd, ZooKeeper, Spanner, ESPN, GitHub, Slack, Planet Scale, CockroachDB, Cassandra

**Looking Ahead: Chapters 10-12**

Now that we understand consistency and consensus, the next chapters will be more applied:

**Chapter 10: Batch Processing** - MapReduce and distributed computation (more practical, less theoretical!)  
**Chapter 11: Stream Processing** - Real-time data processing with exactly-once semantics  
**Chapter 12: Future of Data Systems** - Written in 2017, let's see what predictions came true!
