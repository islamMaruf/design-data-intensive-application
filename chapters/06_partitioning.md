# Chapter 6: Partitioning (Sharding)

## Introduction: Breaking Data Into Pieces

Imagine you're Instagram. You have 2 billion users posting photos every day. Can you store all photos on one database server?

**Reality Check**:
- One server's capacity: ~10 TB storage, ~100K queries/second
- Instagram's needs: Petabytes of storage, millions of queries/second

**Solution**: **Partitioning** (also called **sharding**) - split your data across multiple machines.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SINGLE DATABASE (Can't scale!)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Single Server]                               â”‚
â”‚  - 10 TB storage                               â”‚
â”‚  - 100K queries/sec                            â”‚
â”‚  - All users: A-Z                              â”‚
â”‚                                                â”‚
â”‚  ğŸ’¥ Too much data!                             â”‚
â”‚  ğŸ’¥ Too many queries!                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â†“ PARTITION â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PARTITIONED DATABASE (Scales!)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Server 1]      [Server 2]      [Server 3]   â”‚
â”‚  Users: A-H      Users: I-P      Users: Q-Z   â”‚
â”‚  3.3 TB          3.3 TB           3.3 TB       â”‚
â”‚  33K queries/s   33K queries/s    33K queries/sâ”‚
â”‚                                                â”‚
â”‚  âœ… Total: 10 TB, 100K queries/sec            â”‚
â”‚  âœ… Add more servers = more capacity!         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Goals of Partitioning**:
1. **Scalability**: Distribute data and query load across many machines
2. **Performance**: Each node handles a fraction of the data

**Note**: Partitioning is usually combined with replication (covered in Chapter 5).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARTITIONING + REPLICATION              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  Partition 1 (Users A-H):                â”‚
â”‚  [Leader] â†’ [Follower] â†’ [Follower]     â”‚
â”‚                                          â”‚
â”‚  Partition 2 (Users I-P):                â”‚
â”‚  [Leader] â†’ [Follower] â†’ [Follower]     â”‚
â”‚                                          â”‚
â”‚  Partition 3 (Users Q-Z):                â”‚
â”‚  [Leader] â†’ [Follower] â†’ [Follower]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each partition is replicated for fault tolerance!

## Part 1: Partitioning of Key-Value Data

The fundamental question: **Given a key, which partition should it go to?**

### Strategy 1: Partitioning by Key Range

Assign continuous ranges of keys to each partition, like an encyclopedia.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      RANGE PARTITIONING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Partition 1: Keys A to F                      â”‚
â”‚  [Aardvark, Apple, Banana, ..., Fox]           â”‚
â”‚                                                â”‚
â”‚  Partition 2: Keys G to P                      â”‚
â”‚  [Giraffe, House, India, ..., Penguin]         â”‚
â”‚                                                â”‚
â”‚  Partition 3: Keys Q to Z                      â”‚
â”‚  [Queen, Rabbit, Sun, ..., Zebra]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real-World Example - Google Bigtable**:

Bigtable (used by Google Search, Gmail, Google Maps) partitions by row key ranges:

```
Partition 1: Keys "" to "g"
Partition 2: Keys "g" to "p"
Partition 3: Keys "p" to "~"
```

Each partition is called a **tablet**.

**Advantages**:

âœ… **Range queries are efficient**

```sql
-- Find all users whose names start with 'Al'
SELECT * FROM users WHERE name BETWEEN 'Al' AND 'Az';

-- Query goes to ONE partition (A-F)!
```

âœ… **Keys are kept sorted**
- Useful for iterating in order
- Good for time-series data

**Disadvantages**:

âŒ **Risk of hot spots** (uneven distribution)

**Example - Time-Series Data**:

```python
# Sensor data with timestamp as key
# 2024-01-15-00:00:00: temp=20
# 2024-01-15-00:00:01: temp=21
# 2024-01-15-00:00:02: temp=22
# ...

Partitions by date:
  Partition 1: 2024-01-01 to 2024-01-31
  Partition 2: 2024-02-01 to 2024-02-28
  Partition 3: 2024-03-01 to 2024-03-31
```

**Problem**: All writes go to the current month's partition!

```
[Partition 1] (Jan - cold ğŸ¥¶)
[Partition 2] (Feb - cold ğŸ¥¶)
[Partition 3] (Mar - HOT ğŸ”¥ğŸ”¥ğŸ”¥) â† All writes here!
```

**Solution**: Add a prefix to distribute the load

```python
# Instead of: 2024-03-15-10:30:00
# Use: sensor_id:2024-03-15-10:30:00

# Examples:
sensor_123:2024-03-15-10:30:00 â†’ Partition 1
sensor_456:2024-03-15-10:30:00 â†’ Partition 2
sensor_789:2024-03-15-10:30:00 â†’ Partition 3
```

Now writes are distributed across all partitions!

**Real-World Example - HBase**:

HBase (Hadoop database) faced this exact problem. They now recommend prefixing row keys to avoid hot spots.

### Strategy 2: Partitioning by Hash of Key

Apply a hash function to the key, then partition by hash value.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      HASH PARTITIONING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  hash("Alice") = 0x1A3F â†’ Partition 1          â”‚
â”‚  hash("Bob") = 0x7C2E â†’ Partition 2            â”‚
â”‚  hash("Charlie") = 0x4B91 â†’ Partition 2        â”‚
â”‚  hash("Diana") = 0x2F17 â†’ Partition 1          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Process:
1. Hash the key: hash(key) â†’ number
2. Partition: number % num_partitions â†’ partition_id
```

**Example**:

```python
import hashlib

def get_partition(key, num_partitions):
    # Hash the key
    hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    # Determine partition
    partition_id = hash_value % num_partitions
    
    return partition_id

# Example usage:
print(get_partition("Alice", 3))    # Output: 2
print(get_partition("Bob", 3))      # Output: 0
print(get_partition("Charlie", 3))  # Output: 1
```

**Advantages**:

âœ… **Even distribution**: Hash function distributes keys uniformly

```
Before hashing (by name):
[A-F]: 10000 users
[G-P]: 15000 users
[Q-Z]: 5000 users
âŒ Unbalanced!

After hashing:
[Partition 1]: 10000 users
[Partition 2]: 10000 users
[Partition 3]: 10000 users
âœ… Balanced!
```

âœ… **No hot spots**: Writes are evenly distributed

**Disadvantages**:

âŒ **Range queries are impossible**

```sql
-- Want all users with names A-C
SELECT * FROM users WHERE name BETWEEN 'A' AND 'C';

-- With hash partitioning:
hash("A...") could be in ANY partition
hash("B...") could be in ANY partition  
hash("C...") could be in ANY partition

-- Must scan ALL partitions! ğŸ’¥
```

âŒ **Lost key ordering**: Can't iterate through sorted keys

**Hash Functions**:

Good hash functions:
- **MD5**, **SHA-1**: Cryptographic hashes (overkill, slow)
- **Murmur3**, **xxHash**: Fast, non-cryptographic hashes âœ…

**Bad idea**: Language built-in hash (Java's `hashCode`, Python's `hash`)
- Not consistent across processes/machines
- May change between language versions

**Real-World Examples**:

- **Cassandra**: Uses Murmur3 hash for partitioning
- **MongoDB**: Uses MD5 hash for sharded collections
- **Redis Cluster**: Uses CRC16 hash

### Strategy 3: Consistent Hashing

**Problem with Simple Hash Partitioning**:

```python
# 3 partitions initially
get_partition("Alice", 3)  # â†’ Partition 0

# Add one more partition (now 4)
get_partition("Alice", 4)  # â†’ Partition 2 âŒ

# "Alice"'s data moved! Must rebalance almost ALL data!
```

**Consistent Hashing** minimizes data movement when partitions change.

**How It Works**:

Imagine a ring (circle) of hash values from 0 to 2^32-1.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CONSISTENT HASHING RING                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚                 0 (top)                        â”‚
â”‚                  â”‚                             â”‚
â”‚         [Node A] â”‚                             â”‚
â”‚              â†˜   â”‚   â†™ [Node B]               â”‚
â”‚    2^31 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2^30               â”‚
â”‚              â†—       â†–                         â”‚
â”‚         [Node C]                               â”‚
â”‚                  â”‚                             â”‚
â”‚               2^31 (bottom)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each node responsible for range from previous node to itself
Node A: hash values from Node C to Node A
Node B: hash values from Node A to Node B  
Node C: hash values from Node B to Node C
```

**Finding Partition**:

```python
def get_partition_consistent(key, nodes):
    key_hash = hash(key)
    
    # Find first node with hash â‰¥ key_hash (clockwise)
    for node in sorted(nodes, key=lambda n: hash(n)):
        if hash(node) >= key_hash:
            return node
    
    # Wrap around: return first node
    return nodes[0]
```

**Adding a Node**:

```
BEFORE (3 nodes):
  Node A: 0 to 1000
  Node B: 1000 to 2000
  Node C: 2000 to 3000

ADD Node D at position 1500:
  Node A: 0 to 1000      (unchanged âœ…)
  Node D: 1000 to 1500   (NEW)
  Node B: 1500 to 2000   (only half moved)
  Node C: 2000 to 3000   (unchanged âœ…)

Only 1/4 of data moved! (Instead of 3/4 with simple hashing)
```

**Real-World Usage**:
- **Amazon Dynamo**: Original consistent hashing paper
- **Cassandra**: Uses consistent hashing with virtual nodes
- **Riak**: Consistent hashing
- **CDNs** (Content Delivery Networks): Akamai, CloudFlare

**Virtual Nodes**:

Problem: Physical nodes might not distribute evenly on the ring.

Solution: Each physical node responsible for multiple virtual nodes.

```
Physical Nodes: A, B, C

Virtual Nodes:
Ring position 100: A1
Ring position 300: B1
Ring position 500: C1
Ring position 700: A2
Ring position 900: B2
Ring position 1100: C2
...

Each physical node handles multiple ranges!
More even distribution âœ…
```

## Part 2: Partitioning and Secondary Indexes

Secondary indexes make partitioning much more complicated!

**Background**: Secondary indexes let you query by non-key attributes.

```sql
-- Primary key query (easy with partitioning)
SELECT * FROM users WHERE user_id = 12345;
â†’ Hash(12345) â†’ Partition 2

-- Secondary index query (hard!)
SELECT * FROM users WHERE age = 25;
â†’ Users with age=25 could be in ANY partition! ğŸ’¥
```

### Approach 1: Partitioning by Document (Local Indexes)

Each partition maintains its own secondary indexes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DOCUMENT-PARTITIONED INDEXES                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Partition 1 (Users 0-999):                    â”‚
â”‚  Primary: [user_id â†’ data]                     â”‚
â”‚  Index: age=25 â†’ [user_123, user_456]          â”‚
â”‚         age=30 â†’ [user_789]                    â”‚
â”‚                                                â”‚
â”‚  Partition 2 (Users 1000-1999):                â”‚
â”‚  Primary: [user_id â†’ data]                     â”‚
â”‚  Index: age=25 â†’ [user_1234, user_1567]        â”‚
â”‚         age=30 â†’ [user_1890]                   â”‚
â”‚                                                â”‚
â”‚  Partition 3 (Users 2000-2999):                â”‚
â”‚  Primary: [user_id â†’ data]                     â”‚
â”‚  Index: age=25 â†’ [user_2345]                   â”‚
â”‚         age=30 â†’ [user_2678, user_2901]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Querying**:

```sql
SELECT * FROM users WHERE age = 25;

-- Must query ALL partitions (scatter/gather):
results = []
for partition in all_partitions:
    results += partition.query("age = 25")
return results
```

```
Query Process:
Client â†’ [Partition 1] â†’ Returns [user_123, user_456]
Client â†’ [Partition 2] â†’ Returns [user_1234, user_1567]
Client â†’ [Partition 3] â†’ Returns [user_2345]

Client merges: [user_123, user_456, user_1234, user_1567, user_2345]
```

**Advantages**:
- âœ… Writes are fast (only update one partition's index)

**Disadvantages**:
- âŒ Reads are slow (must query all partitions)
- âŒ Called "scatter/gather" - expensive!

**Real-World Usage**:
- **MongoDB**: Local secondary indexes
- **Cassandra**: Local indexes
- **Elasticsearch**: Each shard has its own index

**Real-World Example - Elasticsearch**:

```python
# Elasticsearch: 3 shards, searching for "python tutorial"
GET /articles/_search
{
  "query": {
    "match": {"content": "python tutorial"}
  }
}

# Process:
# 1. Query sent to all 3 shards
# 2. Each shard searches its local index
# 3. Results merged and scored
# 4. Top results returned

# If you have 100 shards â†’ 100 queries per search! ğŸ’¥
```

### Approach 2: Partitioning by Term (Global Indexes)

Create a global secondary index, partitioned separately from the primary data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    TERM-PARTITIONED INDEXES                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Data Partitions (by user_id):                 â”‚
â”‚  [Partition 1]: Users 0-999                    â”‚
â”‚  [Partition 2]: Users 1000-1999                â”‚
â”‚  [Partition 3]: Users 2000-2999                â”‚
â”‚                                                â”‚
â”‚  Index Partitions (by age):                    â”‚
â”‚  [Index A]: ages 0-33                          â”‚
â”‚    age=25 â†’ [user_123, user_456, user_1234,   â”‚
â”‚               user_1567, user_2345]            â”‚
â”‚  [Index B]: ages 34-66                         â”‚
â”‚    age=50 â†’ [user_789, user_1890, ...]        â”‚
â”‚  [Index C]: ages 67-99                         â”‚
â”‚    age=80 â†’ [user_901, ...]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Querying**:

```sql
SELECT * FROM users WHERE age = 25;

-- Query ONE index partition:
1. hash(25) â†’ Index partition A
2. Index A returns: [user_123, user_456, user_1234, user_1567, user_2345]
3. Fetch from data partitions (multiple queries)
```

**Advantages**:
- âœ… Reads are faster (query one index partition, not all)
- âœ… More efficient for queries

**Disadvantages**:
- âŒ Writes are slower (must update index partition separately)
- âŒ Asynchronous updates: Index might be slightly stale

**Write Process**:

```
1. Client writes: user_id=1234, age=25
2. Write to data partition (hash(1234) â†’ Partition 2)
3. Write to index partition (hash(25) â†’ Index A)
   â†‘
   This is often ASYNCHRONOUS!
```

**Real-World Usage**:
- **DynamoDB**: Global secondary indexes
- **Riak**: Global indexes with async updates

**Real-World Example - Amazon DynamoDB**:

```python
# DynamoDB table partitioned by user_id
# Global Secondary Index on email

# Write user
dynamodb.put_item(
    TableName='Users',
    Item={
        'user_id': '12345',
        'name': 'Alice',
        'email': 'alice@example.com'
    }
)

# Query by email (uses global index)
response = dynamodb.query(
    TableName='Users',
    IndexName='email-index',  # Uses global secondary index
    KeyConditionExpression='email = :email',
    ExpressionAttributeValues={
        ':email': 'alice@example.com'
    }
)

# Behind the scenes:
# 1. Query goes to one partition of the email-index
# 2. Index returns user_id=12345
# 3. Fetch from main table using user_id
```

**Warning**: Global indexes in DynamoDB are eventually consistent! After a write, the index might take milliseconds to update.

## Part 3: Rebalancing Partitions

**Rebalancing**: Moving data between nodes when you add/remove nodes.

**Requirements**:
1. Load should be shared fairly across nodes
2. Database should continue accepting reads/writes during rebalancing
3. Minimize data movement (expensive!)

### Why Rebalance?

**Scenario 1: Add More Nodes**
```
Before: 3 nodes, 30 TB total
[Node 1: 10 TB] [Node 2: 10 TB] [Node 3: 10 TB]

Add Node 4:
Want: 4 nodes, 7.5 TB each
[Node 1: 7.5 TB] [Node 2: 7.5 TB] [Node 3: 7.5 TB] [Node 4: 7.5 TB]

Must move 7.5 TB to Node 4!
```

**Scenario 2: Remove Failed Node**
```
Before: 4 nodes
[Node 1] [Node 2] [Node 3] [Node 4: ğŸ’¥ Failed]

After:
[Node 1] [Node 2] [Node 3]
Must redistribute Node 4's data!
```

**Scenario 3: Uneven Load**
```
[Node 1: 5 TB, 10K qps]   â† Underutilized
[Node 2: 15 TB, 50K qps]  â† Overloaded! ğŸ”¥
[Node 3: 10 TB, 20K qps]  â† Normal

Rebalance to distribute load evenly
```

### Strategy 1: Fixed Number of Partitions

Create many more partitions than nodes from the start.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FIXED PARTITIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Configuration: 12 partitions, 3 nodes         â”‚
â”‚                                                â”‚
â”‚  Initial:                                      â”‚
â”‚  Node 1: [P1, P2, P3, P4]                     â”‚
â”‚  Node 2: [P5, P6, P7, P8]                     â”‚
â”‚  Node 3: [P9, P10, P11, P12]                  â”‚
â”‚                                                â”‚
â”‚  Add Node 4:                                   â”‚
â”‚  Node 1: [P1, P2, P3]      â† Moved P4         â”‚
â”‚  Node 2: [P5, P6, P7]      â† Moved P8         â”‚
â”‚  Node 3: [P9, P10, P11]    â† Moved P12        â”‚
â”‚  Node 4: [P4, P8, P12]     â† NEW              â”‚
â”‚                                                â”‚
â”‚  Only moved 3 partitions!                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Process**:
1. Pick partitions to move (usually from high-load nodes)
2. Copy data to new node
3. Switch traffic to new node
4. Delete old copy

**Choosing Number of Partitions**:

Rule of thumb: 10-100 partitions per node

```
10 nodes: 100-1000 partitions
100 nodes: 1000-10000 partitions
```

**Too few partitions**: Can't rebalance effectively
**Too many partitions**: Overhead from managing many partitions

**Real-World Example - Riak**:

Riak uses 64 partitions (called vnodes) by default per physical node.

```python
# 3 physical nodes:
Node A: vnodes 0-63
Node B: vnodes 64-127
Node C: vnodes 128-191

# Total: 192 partitions

# Add Node D:
Node A: vnodes 0-47       (kept 48, gave 16)
Node B: vnodes 64-111     (kept 48, gave 16)
Node C: vnodes 128-175    (kept 48, gave 16)
Node D: vnodes 48-63, 112-127, 176-191  (received 48)
```

### Strategy 2: Dynamic Partitioning

Start with small number of partitions, split when they get too large.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DYNAMIC PARTITIONING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Initial: 1 partition                          â”‚
â”‚  [Partition 1: 0 GB]                           â”‚
â”‚                                                â”‚
â”‚  Partition grows:                              â”‚
â”‚  [Partition 1: 10 GB]                          â”‚
â”‚                                                â”‚
â”‚  Split at 10 GB threshold:                     â”‚
â”‚  [Partition 1: 5 GB] [Partition 2: 5 GB]      â”‚
â”‚                                                â”‚
â”‚  Partition 1 grows again:                      â”‚
â”‚  [Partition 1: 10 GB] [Partition 2: 5 GB]     â”‚
â”‚                                                â”‚
â”‚  Split again:                                  â”‚
â”‚  [P1: 5GB] [P3: 5GB] [P2: 5GB]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Advantages**:
- âœ… Adapts to data volume automatically
- âœ… No need to choose partition count upfront

**Disadvantages**:
- âŒ Empty database has only 1 partition (can't distribute initial load)

**Solution: Pre-splitting**

```python
# HBase example: Create table with pre-split regions
create 'users', 'info', SPLITS => ['100', '200', '300', '400']

# Creates 5 regions:
# Region 1: '' to '100'
# Region 2: '100' to '200'
# Region 3: '200' to '300'
# Region 4: '300' to '400'
# Region 5: '400' to ''
```

**Real-World Usage**:
- **HBase**: Dynamic region splitting
- **RethinkDB**: Automatic sharding

**Real-World Example - MongoDB Auto-Sharding**:

```javascript
// MongoDB: Enable sharding
sh.enableSharding("mydb")

// Shard collection by user_id
sh.shardCollection("mydb.users", {user_id: 1})

// Initially: 1 chunk (partition)
// As data grows:
//   Chunk 1: user_id [-âˆ to 1000] (2 GB)
//   â†’ Split into:
//      Chunk 1: [-âˆ to 500] (1 GB)
//      Chunk 2: [500 to 1000] (1 GB)

// MongoDB automatically splits chunks > 64 MB
// and balances chunks across shards
```

### Strategy 3: Partitioning Proportional to Nodes

Fix the number of partitions per node.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PROPORTIONAL PARTITIONING                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Rule: 10 partitions per node                  â”‚
â”‚                                                â”‚
â”‚  3 nodes:                                      â”‚
â”‚  30 partitions total                           â”‚
â”‚  Node 1: [P1...P10]                            â”‚
â”‚  Node 2: [P11...P20]                           â”‚
â”‚  Node 3: [P21...P30]                           â”‚
â”‚                                                â”‚
â”‚  Add Node 4:                                   â”‚
â”‚  40 partitions total                           â”‚
â”‚  - Create 10 new partitions                    â”‚
â”‚  - Randomly split existing partitions          â”‚
â”‚  - Move half to new node                       â”‚
â”‚                                                â”‚
â”‚  Node 1: [P1...P10]                            â”‚
â”‚  Node 2: [P11...P20]                           â”‚
â”‚  Node 3: [P21...P30]                           â”‚
â”‚  Node 4: [P31...P40] â† New partitions          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Advantage**:
- âœ… Partition size remains stable as cluster grows

**Used by**: Cassandra 3.0+

### Automatic vs Manual Rebalancing

**Automatic Rebalancing**:
- System decides when and how to move data
- Convenient but risky

**Risk**: Rebalancing is expensive (network, disk I/O). Automatic rebalancing during peak traffic can make things worse!

**Real-World Disaster**:

```
Scenario: E-commerce site during Black Friday

11:00 AM: High traffic, servers at 80% CPU
11:15 AM: One node slows down (garbage collection pause)
11:16 AM: Auto-rebalancer detects slow node, starts moving data
11:17 AM: Network saturated with rebalancing traffic
11:18 AM: All nodes slow down due to network contention
11:19 AM: System cascade failure ğŸ’¥ğŸ’¥ğŸ’¥

Customers can't check out!
Millions in lost revenue!
```

**Manual Rebalancing**:
- Operator manually triggers rebalancing
- More work but safer

**Best Practice**: Use semi-automatic
- System suggests rebalancing
- Operator approves and schedules it during low-traffic period

**Real-World Example - Couchbase**:

```
Couchbase UI:
  "Cluster is unbalanced. 
   Node 1: 15% of data
   Node 2: 45% of data
   Node 3: 40% of data
   
   [Rebalance] button
   
   Note: Rebalancing may impact performance.
   Schedule during maintenance window."
```

## Part 4: Request Routing (Service Discovery)

**Problem**: Client wants to read `user_id=12345`. Which node should it connect to?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SERVICE DISCOVERY PROBLEM                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Client: "Where is user_id=12345?"             â”‚
â”‚                                                â”‚
â”‚  Cluster:                                      â”‚
â”‚  Node 1: user_ids 0-999                        â”‚
â”‚  Node 2: user_ids 1000-1999                    â”‚
â”‚  Node 3: user_ids 2000-2999                    â”‚
â”‚  Node 4: user_ids 3000-3999                    â”‚
â”‚                                                â”‚
â”‚  Answer: Node 4!                               â”‚
â”‚  But how does client find out?                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Approach 1: Allow Clients to Contact Any Node

Client connects to any node. If it's the wrong node, forward the request.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REQUEST FORWARDING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Step 1: Client â†’ Node 2                       â”‚
â”‚  "Get user_id=12345"                           â”‚
â”‚                                                â”‚
â”‚  Step 2: Node 2 checks                         â”‚
â”‚  "12345 is in Node 4's range, not mine"        â”‚
â”‚                                                â”‚
â”‚  Step 3: Node 2 â†’ Node 4                       â”‚
â”‚  "Get user_id=12345"                           â”‚
â”‚                                                â”‚
â”‚  Step 4: Node 4 â†’ Node 2 â†’ Client              â”‚
â”‚  Returns data                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Simple for clients (connect to any node)

**Cons**:
- âŒ Extra network hop (latency)

**Used by**: Cassandra, Riak

**Cassandra Implementation**:

Every Cassandra node knows about all other nodes (gossip protocol).

```python
# Client connects to any node
client = Cluster(['node1.example.com']).connect()

# Query
result = client.execute("SELECT * FROM users WHERE user_id = 12345")

# Behind the scenes:
# 1. node1 receives request
# 2. node1 knows user_id=12345 is on node4 (via gossip)
# 3. node1 forwards to node4
# 4. node4 returns data to node1
# 5. node1 returns to client

# Future requests: Client learns node4 has that data,
# connects directly to node4 (optimization)
```

### Approach 2: Routing Tier

Dedicated load balancer routes requests to correct partition.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUTING TIER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Client â†’ [Load Balancer/Router]               â”‚
â”‚              â†“         â†“        â†“              â”‚
â”‚           [Node 1] [Node 2] [Node 3]           â”‚
â”‚                                                â”‚
â”‚  Router knows partition mapping:               â”‚
â”‚  user_id 0-999 â†’ Node 1                        â”‚
â”‚  user_id 1000-1999 â†’ Node 2                    â”‚
â”‚  user_id 2000-2999 â†’ Node 3                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… No extra hop (router sends to correct node)
- âœ… Clients are simple (always connect to router)

**Cons**:
- âŒ Router is single point of failure
- âŒ Router can become bottleneck

**Used by**: Many systems with HAProxy, nginx

**Real-World Example - MongoDB**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MONGODB SHARDED CLUSTER                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Application â†’ [mongos] (query router)         â”‚
â”‚                    â†“                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â†“          â†“          â†“               â”‚
â”‚    [Shard 1]  [Shard 2]  [Shard 3]            â”‚
â”‚                                                â”‚
â”‚  mongos maintains routing table:               â”‚
â”‚  Collection: users                             â”‚
â”‚    {user_id: MinKey} â†’ {user_id: 1000} â†’ Shard 1 â”‚
â”‚    {user_id: 1000} â†’ {user_id: 2000} â†’ Shard 2   â”‚
â”‚    {user_id: 2000} â†’ {user_id: MaxKey} â†’ Shard 3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Application connects to `mongos`, which routes queries to the right shard.

### Approach 3: Partition-Aware Clients

Client itself knows partition mapping.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARTITION-AWARE CLIENT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Client has routing table:                     â”‚
â”‚  user_id 0-999 â†’ node1.example.com             â”‚
â”‚  user_id 1000-1999 â†’ node2.example.com         â”‚
â”‚  user_id 2000-2999 â†’ node3.example.com         â”‚
â”‚                                                â”‚
â”‚  Client calculates:                            â”‚
â”‚  user_id=12345 â†’ node4.example.com             â”‚
â”‚                                                â”‚
â”‚  Client directly connects to correct node!     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros**:
- âœ… Lowest latency (direct connection)
- âœ… No routing tier needed

**Cons**:
- âŒ Complex client library
- âŒ Client must track partition changes

**Used by**: Some NoSQL drivers (Cassandra, Riak client libraries)

### Coordination Services: ZooKeeper

**Problem**: How do nodes/routers/clients know the current partition mapping?

**Solution**: Use a coordination service like **ZooKeeper** or **etcd**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ZOOKEEPER-BASED ROUTING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚         [ZooKeeper Cluster]                    â”‚
â”‚         Stores: Partition mapping              â”‚
â”‚                  â”‚                             â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚       â†“          â†“          â†“                  â”‚
â”‚   [Node 1]   [Node 2]   [Node 3]              â”‚
â”‚       â†‘          â†‘          â†‘                  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                  â”‚                             â”‚
â”‚            Notify changes                      â”‚
â”‚                  â”‚                             â”‚
â”‚       [Routing Tier / Client]                  â”‚
â”‚       Subscribes to changes                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ZooKeeper Stores**:
- Which partitions are on which nodes
- Which node is the leader for each partition
- When nodes join/leave

**Process**:

1. **Nodes register with ZooKeeper**
```python
# Node 1 starts up
zk.create("/nodes/node1", "alive", ephemeral=True)
zk.create("/partitions/p1", "node1")
zk.create("/partitions/p2", "node1")
```

2. **Router/Client watches ZooKeeper**
```python
def on_partition_change(partitions):
    # Update local routing table
    routing_table = build_routing_table(partitions)

zk.watch("/partitions", on_partition_change)
```

3. **Partition moves**
```python
# Rebalancing: Move partition 5 from node2 to node4
zk.set("/partitions/p5", "node4")
# â†“
# ZooKeeper notifies all watchers
# â†“  
# Routers/clients update their routing tables
```

**Real-World Usage**:
- **HBase**: Uses ZooKeeper to track region servers
- **Kafka**: Uses ZooKeeper for topic partition assignment (moving to internal metadata)
- **MongoDB**: Uses its own internal version (config servers)

## Part 5: Real-World Examples

### Example 1: Instagram's Sharding Journey

**Initial (2010)**: Single PostgreSQL database

**Problem (2011)**: 10 million users, database overloaded

**Solution**: Sharded by user_id
```
hash(user_id) % 1000 â†’ Shard ID

Initially: 1000 logical shards, 100 physical servers
Each server: 10 logical shards
```

**Why 1000 logical shards, not 100?**
- Can split physical servers easily
- Server 1 with shards [0-9] â†’ Split to two servers: [0-4] and [5-9]

**User ID Generation**:
```python
# Instagram snowflake-style IDs
# |--- 41 bits: timestamp ---|-- 13 bits: shard_id --|-- 10 bits: sequence --|

user_id = (timestamp << 23) | (shard_id << 10) | sequence
```

User ID encodes which shard it's on! Makes lookup O(1).

**Result**: Scaled to 1 billion users by 2018

### Example 2: Discord's Message Storage

**Challenge**: Billions of messages, need fast access to recent messages in each channel.

**Partitioning Strategy**: By channel_id and timestamp

```python
# Partition key: (channel_id, bucket)
# bucket = timestamp // BUCKET_SIZE

message_id = (timestamp << 22) | (shard_id << 12) | sequence
partition_key = (channel_id, message_id // BUCKET_SIZE)
```

**Benefits**:
- Recent messages in same channel are co-located
- Range queries are efficient: "Get last 50 messages in channel"

**Technology**: Cassandra with custom partitioning

### Example 3: Uber's Schemaless (Docstore)

**Challenge**: Different cities have different access patterns

**Partitioning**: Multi-level
1. First level: By city
2. Second level: By entity type (users, trips, payments)
3. Third level: By entity_id

```
Partition: city + entity_type + hash(entity_id)

Examples:
SF + users + hash(user_123) â†’ Partition A
NYC + trips + hash(trip_456) â†’ Partition B
```

**Benefits**:
- Can scale each city independently
- Can prioritize high-value cities (more resources)
- Data locality (city data stays together)

**Result**: Supports 18 million trips per day across 600+ cities

## Summary

**Key Takeaways**:

1. **Partitioning Strategies**
   - **Range partitioning**: Good for range queries, risk of hot spots
   - **Hash partitioning**: Even distribution, can't do range queries
   - **Consistent hashing**: Minimizes data movement during rebalancing

2. **Secondary Indexes**
   - **Document-partitioned**: Fast writes, slow reads (scatter/gather)
   - **Term-partitioned**: Fast reads, slower writes (async updates)

3. **Rebalancing**
   - **Fixed partitions**: Simple, need to choose count upfront
   - **Dynamic**: Adapts automatically, empty database is slow
   - **Proportional**: Stable partition sizes

4. **Request Routing**
   - **Contact any node**: Simple, extra hop
   - **Routing tier**: Clean separation, potential bottleneck
   - **Partition-aware clients**: Fastest, complex clients
   - **ZooKeeper**: Coordination for tracking partition changes

5. **Real-World Wisdom**
   - Start simple (range or hash partitioning)
   - Monitor hot spots constantly
   - Be very careful with automatic rebalancing
   - Over-provision partitions (easier to rebalance)
   - Test rebalancing in staging first!

**Next Chapter**: Transactions - how to ensure correctness when multiple operations must succeed or fail together.
