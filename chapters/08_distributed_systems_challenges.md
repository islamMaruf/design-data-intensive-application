# Chapter 8: The Trouble with Distributed Systems

## Introduction: Welcome to the Real World

You've built a beautiful database system running on one machine. It has perfect ACID transactions, consistent data, and reliable operations. Life is good! ğŸ˜Š

Then your boss says: "We need to scale. Split it across 10 servers."

Suddenly, everything that could go wrong, **will** go wrong:
- ğŸŒ **Networks fail** (cables unplugged, switches crash, packets dropped)
- â° **Clocks drift** (servers disagree on what time it is)
- ğŸ’¥ **Machines crash** (power failures, hardware faults)
- ğŸŒ **Processes pause** (garbage collection, OS suspends threads)
- ğŸ“¨ **Messages get lost** (network congestion, buffer overflows)
- ğŸ”„ **Messages arrive out of order** (different network paths)
- ğŸ¢ **Operations are slow** (sometimes fast, sometimes slow, unpredictable)

**Welcome to distributed systems** - where Murphy's Law is an understatement!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SINGLE MACHINE vs DISTRIBUTED SYSTEM          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Single Machine:                               â”‚
â”‚  [CPU] â”€â”€fast reliable busâ”€â”€â†’ [Memory]        â”‚
â”‚  Either works or crashes completely           â”‚
â”‚  Time is consistent                            â”‚
â”‚  Operations are fast and predictable           â”‚
â”‚                                                â”‚
â”‚  Distributed System:                           â”‚
â”‚  [Server A] â”€â”€unreliable networkâ”€â”€â†’ [Server B]â”‚
â”‚  Partial failures (A works, B crashes)         â”‚
â”‚  Clocks disagree                               â”‚
â”‚  Operations are slow and unpredictable         â”‚
â”‚  Messages lost, delayed, duplicated            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This chapter explores everything that can go wrong in distributed systems, and how to build systems that work despite these problems.

## Part 1: Faults and Partial Failures

### The Fundamental Problem

**In a single computer**:
- Either it works correctly OR
- It fails completely (crash, blue screen)

**In a distributed system**:
- Some parts work correctly AND
- Some parts fail simultaneously
- **Partial failures** are nondeterministic (random, unpredictable)

**Real-World Analogy**:

```
Single Computer = Light Switch
  ON â†’ Everything works âœ…
  OFF â†’ Everything fails âŒ
  Simple, predictable!

Distributed System = Christmas Lights
  Some bulbs work âœ…
  Some bulbs broken âŒ
  Working bulbs keep working
  You don't know which are broken until you check
  Complex, unpredictable!
```

### Example: Sending a Request

Simple request from Client to Server. What can go wrong?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REQUEST/RESPONSE SCENARIOS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Scenario 1: Success âœ…                        â”‚
â”‚  Client â”€â”€requestâ”€â”€â†’ Server                    â”‚
â”‚  Client â†â”€responseâ”€â”€ Server                    â”‚
â”‚                                                â”‚
â”‚  Scenario 2: Request lost ğŸ“§âŒ                  â”‚
â”‚  Client â”€â”€requestâ”€â”€X                           â”‚
â”‚  (Server never receives it)                    â”‚
â”‚                                                â”‚
â”‚  Scenario 3: Server crashes ğŸ’¥                 â”‚
â”‚  Client â”€â”€requestâ”€â”€â†’ Server ğŸ’¥                 â”‚
â”‚  (No response)                                 â”‚
â”‚                                                â”‚
â”‚  Scenario 4: Response lost ğŸ“¨âŒ                 â”‚
â”‚  Client â”€â”€requestâ”€â”€â†’ Server                    â”‚
â”‚  Server processes request âœ…                   â”‚
â”‚  Client â†â”€â”€â”€â”€â”€â”€X (response lost)               â”‚
â”‚                                                â”‚
â”‚  Scenario 5: Response delayed ğŸ¢               â”‚
â”‚  Client â”€â”€requestâ”€â”€â†’ Server                    â”‚
â”‚  (Long pause...)                               â”‚
â”‚  Client â†â”€responseâ”€â”€ (finally arrives)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Problem**: From client's perspective, scenarios 2, 3, 4, and 5 all look the same - **no response**!

```python
# Client code
def make_request(server_url, data):
    try:
        response = http.post(server_url, data, timeout=5)
        return response
    except Timeout:
        # What happened?
        # - Request lost? (retry is safe)
        # - Server crashed? (retry is safe)
        # - Response lost? (retry might duplicate! âŒ)
        # - Server just slow? (retry might duplicate! âŒ)
        # 
        # We can't tell! ğŸ¤·
        pass
```

**Key Insight**: In distributed systems, you often can't distinguish between different types of failures. This uncertainty is fundamental and unavoidable.

### Two Philosophies: Cloud Computing vs Supercomputing

**Supercomputing** (HPC - High-Performance Computing):
- Thousands of CPUs, tightly coupled
- Checkpoint entire system state periodically
- If any node fails â†’ **Stop everything**, restore from checkpoint
- Treats partial failure like complete failure

**Cloud Computing**:
- Commodity hardware, loose coupling
- Nodes fail independently
- System continues operating despite failures
- Built for partial failures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FAILURE HANDLING PHILOSOPHIES              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Approach    â”‚  When Node Fails              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Supercompute â”‚  Stop everything              â”‚
â”‚              â”‚  Restore checkpoint           â”‚
â”‚              â”‚  Resume from there            â”‚
â”‚              â”‚  (like save/load game)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cloud        â”‚  Failed node marked dead      â”‚
â”‚              â”‚  Other nodes continue         â”‚
â”‚              â”‚  Reroute traffic              â”‚
â”‚              â”‚  (like highway detour)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This book focuses on cloud computing philosophy** - build systems that tolerate partial failures.

## Part 2: Unreliable Networks

Networks are the foundation of distributed systems. Unfortunately, they're also the most unreliable component!

### Network Faults in Practice

**Reality Check**: Major companies experience frequent network issues.

**Real-World Data**:

1. **Microsoft Azure (2012 Study)**:
   - 5 network failures per month affecting customer-visible services
   - Average downtime: 59 seconds
   - Max downtime: 2.5 hours

2. **Amazon EC2 (Multiple incidents)**:
   - 2011: Networking event caused widespread outages
   - 2012: Network issue took down Netflix, Pinterest, Instagram
   - 2017: S3 outage due to network partition

3. **GitHub (2012)**:
   - Network partition split database cluster
   - Led to data inconsistency
   - Required manual recovery

**Common Network Problems**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TYPES OF NETWORK FAILURES                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  1. Packet Loss                                â”‚
â”‚     [Client] â”€â”€X  (packet dropped)             â”‚
â”‚     - Network congestion                       â”‚
â”‚     - Buffer overflow at switch                â”‚
â”‚     - Faulty network card                      â”‚
â”‚                                                â”‚
â”‚  2. Cable Unplugged                            â”‚
â”‚     [Server] â”€â”€â•³â”€â”€ [Switch]                    â”‚
â”‚     - Someone trips over cable                 â”‚
â”‚     - Maintenance accident                     â”‚
â”‚                                                â”‚
â”‚  3. Network Partition                          â”‚
â”‚     [A] [B] [C] â”€â•³â”€ [D] [E] [F]               â”‚
â”‚     - Two groups can communicate internally    â”‚
â”‚     - Cannot communicate across groups         â”‚
â”‚     - "Split brain" scenario                   â”‚
â”‚                                                â”‚
â”‚  4. Slow Network                               â”‚
â”‚     [Client] â”€â”€â”€â”€ğŸŒâ”€slowâ”€â†’ [Server]           â”‚
â”‚     - Network congestion                       â”‚
â”‚     - Overloaded switch                        â”‚
â”‚     - Bad routing                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detecting Faults: Timeouts

How do you know if a remote node is down?

**Answer**: Use **timeouts**. If no response within X seconds, assume failure.

```python
def call_remote_service(url, timeout=5):
    start_time = time.time()
    try:
        response = http.get(url, timeout=timeout)
        return response
    except Timeout:
        elapsed = time.time() - start_time
        print(f"No response after {elapsed} seconds")
        # Assume service is down
        return None
```

**The Timeout Dilemma**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHOOSING TIMEOUT VALUE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Too Short (e.g., 100ms):                      â”‚
â”‚  âŒ False positives (node just slow)           â”‚
â”‚  âŒ Unnecessary failovers                      â”‚
â”‚  âŒ Cascading failures                         â”‚
â”‚                                                â”‚
â”‚  Too Long (e.g., 60s):                         â”‚
â”‚  âŒ Slow failure detection                     â”‚
â”‚  âŒ Users wait a long time                     â”‚
â”‚  âŒ System unavailable longer                  â”‚
â”‚                                                â”‚
â”‚  Just Right (adaptive):                        â”‚
â”‚  âœ… Based on typical response time             â”‚
â”‚  âœ… Add margin for variance                    â”‚
â”‚  âœ… Adjust based on measurements               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Adaptive Timeout Example**:

```python
class AdaptiveTimeout:
    def __init__(self):
        self.response_times = []
        self.window_size = 100  # Last 100 requests
    
    def record_response_time(self, duration):
        self.response_times.append(duration)
        if len(self.response_times) > self.window_size:
            self.response_times.pop(0)
    
    def get_timeout(self):
        if not self.response_times:
            return 5.0  # Default 5 seconds
        
        # Calculate based on percentiles
        p99 = np.percentile(self.response_times, 99)
        
        # Timeout = 2x p99 response time
        timeout = 2 * p99
        
        # Clamp between 1s and 30s
        return max(1.0, min(30.0, timeout))

# Usage
timeout_manager = AdaptiveTimeout()

for request in requests:
    timeout = timeout_manager.get_timeout()
    start = time.time()
    try:
        response = call_service(url, timeout=timeout)
        duration = time.time() - start
        timeout_manager.record_response_time(duration)
    except Timeout:
        handle_timeout()
```

### Network Congestion and Queueing

Networks don't fail just by breaking - they also fail by getting **slow**.

**Where Delays Happen**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NETWORK DELAY SOURCES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  [Client] â†’ [Queue 1] â†’ [Network] â†’ [Queue 2] â†’ [Server] â”‚
â”‚     â†“          â†“          â†“            â†“         â†“   â”‚
â”‚   App      NIC         Switch        NIC       App  â”‚
â”‚   queue    queue       queue         queue     queueâ”‚
â”‚                                                â”‚
â”‚  1. Application Send Queue                     â”‚
â”‚     - TCP send buffer full                     â”‚
â”‚     - OS waiting to send                       â”‚
â”‚                                                â”‚
â”‚  2. Network Interface Card (NIC) Queue         â”‚
â”‚     - Hardware buffer                          â”‚
â”‚     - Waiting for transmission                 â”‚
â”‚                                                â”‚
â”‚  3. Switch Queue                               â”‚
â”‚     - Multiple inputs, one output              â”‚
â”‚     - Congestion here is common                â”‚
â”‚                                                â”‚
â”‚  4. Receiver NIC Queue                         â”‚
â”‚     - Packets arriving faster than processed   â”‚
â”‚                                                â”‚
â”‚  5. Application Receive Queue                  â”‚
â”‚     - TCP receive buffer                       â”‚
â”‚     - Application processing slowly            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Queueing Delay Example**:

```python
# Network switch processing

packet_queue = []
QUEUE_SIZE = 1000

def receive_packet(packet):
    if len(packet_queue) < QUEUE_SIZE:
        packet_queue.append(packet)
        packet.queue_time_start = time.time()
    else:
        # Queue full - drop packet! ğŸ’¥
        drop_packet(packet)

def forward_packets():
    while packet_queue:
        packet = packet_queue.pop(0)
        
        # Calculate queueing delay
        queue_delay = time.time() - packet.queue_time_start
        
        if queue_delay > 0.1:  # 100ms
            print(f"High queue delay: {queue_delay}s")
        
        transmit(packet)
        time.sleep(0.001)  # 1ms per packet transmission
```

**Real-World Impact - Tail Latency**:

```
Normal situation:
  p50: 10ms
  p99: 50ms
  p99.9: 100ms

During congestion:
  p50: 10ms   (median unchanged!)
  p99: 500ms  (10x worse!)
  p99.9: 5s   (50x worse!)

User impact:
  - Most requests: fine
  - 1 in 100 requests: very slow
  - Service feels "glitchy"
```

**Real-World Example - AWS Network Congestion (2020)**:

During AWS outage:
- Network congestion in single availability zone
- Queue delays reached seconds
- Services timed out waiting for responses
- Cascading failures across multiple services

### Unbounded Delays (No Guarantees)

**Key Insight**: In most networks (including the Internet), there are **no guarantees** on message delivery time.

This is called an **asynchronous network** model.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NETWORK TIMING MODELS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Synchronous Network (rare):                   â”‚
â”‚    - Messages delivered within max time d      â”‚
â”‚    - If not delivered by time d, failed        â”‚
â”‚    - Example: Old telephone networks           â”‚
â”‚    - NOT the Internet!                         â”‚
â”‚                                                â”‚
â”‚  Asynchronous Network (reality):               â”‚
â”‚    - Messages may take arbitrarily long        â”‚
â”‚    - No upper bound on delay                   â”‚
â”‚    - Example: Internet, Ethernet               â”‚
â”‚    - What we deal with!                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why No Guarantees?**

1. **Best-effort delivery**: IP networks don't reserve resources
2. **Shared infrastructure**: Multiple users compete for bandwidth
3. **Variable routing**: Packets take different paths
4. **Economic reasons**: Guaranteed delivery requires expensive infrastructure

**Practical Implication**:

```python
# You CANNOT write this:
def call_service(url):
    response = http.get(url)
    # Assumption: will return within 100ms âŒ
    # Reality: might take 10 seconds, or never return!
    return response

# You MUST write this:
def call_service(url, timeout=5, retries=3):
    for attempt in range(retries):
        try:
            response = http.get(url, timeout=timeout)
            return response
        except Timeout:
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                raise ServiceUnavailable()
```

## Part 3: Unreliable Clocks

Time seems simple - what could go wrong? Turns out, a lot!

### Two Types of Clocks

**1. Time-of-Day Clock** (Wall-Clock Time)

```python
import time

# Time-of-day clock
current_time = time.time()
# Returns: 1704067200.123456 (seconds since Unix epoch: Jan 1, 1970)

# Human readable:
datetime.fromtimestamp(current_time)
# Returns: 2024-01-01 00:00:00
```

**Properties**:
- Returns current date and time
- Synchronized with NTP (Network Time Protocol)
- **Can jump backwards** if clock adjusted!
- **Can jump forwards** if clock adjusted!
- Not suitable for measuring durations

**2. Monotonic Clock** (Steady Clock)

```python
import time

# Monotonic clock
start = time.monotonic()
# Do some work...
end = time.monotonic()
duration = end - start  # Always positive, never jumps backwards
```

**Properties**:
- Always moves forward
- Never jumps backwards or forwards
- **Suitable for measuring durations**
- Not comparable across machines
- Doesn't correspond to wall-clock time

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLOCK COMPARISON                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Time-of-Day      â”‚ Monotonic                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What time is it? â”‚ How long did it take?       â”‚
â”‚ 2:30 PM          â”‚ 5.2 seconds                 â”‚
â”‚ Can jump         â”‚ Always forward              â”‚
â”‚ Comparable acrossâ”‚ Not comparable across       â”‚
â”‚ machines (NTP)   â”‚ machines                    â”‚
â”‚ Use for:         â”‚ Use for:                    â”‚
â”‚ - Timestamps     â”‚ - Timeouts                  â”‚
â”‚ - Logging        â”‚ - Performance measurement   â”‚
â”‚ - Ordering eventsâ”‚ - Durations                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Clock Synchronization Problems

**Problem 1: Clock Drift**

Quartz clocks in computers are not perfect. They drift apart over time.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLOCK DRIFT OVER TIME                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Perfect World:                                â”‚
â”‚  Server A: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (1 second per second)   â”‚
â”‚  Server B: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (1 second per second)   â”‚
â”‚                                                â”‚
â”‚  Reality:                                      â”‚
â”‚  Server A: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (1.0001 sec/sec)       â”‚
â”‚  Server B: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (0.9999 sec/sec)        â”‚
â”‚                                                â”‚
â”‚  After 1 day:                                  â”‚
â”‚  Server A: ahead by 8.6 seconds                â”‚
â”‚  Server B: behind by 8.6 seconds               â”‚
â”‚  Difference: 17.2 seconds! ğŸ’¥                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Typical Clock Drift**:
- Consumer hardware: Â±50 ppm (parts per million)
- That's Â±4.3 seconds per day!

**Problem 2: NTP Synchronization Issues**

NTP (Network Time Protocol) keeps clocks synchronized, but it's not perfect.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NTP SYNCHRONIZATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  [Server] â”€â”€â†’ "What time is it?" â”€â”€â†’ [NTP]    â”‚
â”‚                                       â†“        â”‚
â”‚                                 Look at atomic â”‚
â”‚                                       clock    â”‚
â”‚                                       â†“        â”‚
â”‚  [Server] â†â”€â”€ "It's 14:30:05.123" â†â”€â”€ [NTP]   â”‚
â”‚       â†“                                        â”‚
â”‚   Adjust local clock                           â”‚
â”‚                                                â”‚
â”‚  Problems:                                     â”‚
â”‚  - Network delay (10-100ms typical)            â”‚
â”‚  - Asymmetric routing (request â‰  response)     â”‚
â”‚  - Server load (NTP server busy)               â”‚
â”‚  - Accuracy: Â±35ms on local network            â”‚
â”‚             Â±100ms on public internet          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Problem 3: Clock Jumps**

When NTP detects large drift, it can **jump** the clock.

```python
# Time-of-day clock can jump backwards!

Time: 14:30:00 â†’ Write record with timestamp 14:30:00
Time: 14:29:55 â† NTP adjusts clock backwards 5 seconds! ğŸ’¥
Time: 14:30:00 â†’ Write another record with timestamp 14:30:00

# Two records with same timestamp!
# Or worse: later event has earlier timestamp!
```

**Real-World Disaster - Cloudflare (2016)**:

Cloudflare's load balancer used time-of-day clock to expire sessions.
- NTP jumped clock backwards
- Sessions that should be expired were still considered valid
- Security vulnerabilities!

### Relying on Synchronized Clocks

**Dangerous Assumptions**:

```python
# âŒ WRONG: Assuming clocks are synchronized

def get_global_ordering():
    # Server A writes: timestamp = clock_A.now()
    # Server B writes: timestamp = clock_B.now()
    # 
    # If clock_A > clock_B (due to drift):
    # Event on A appears to happen after event on B
    # Even if A happened first in real time!
    pass

# âŒ WRONG: Using timestamps for causality

def is_after(event1, event2):
    return event1.timestamp > event2.timestamp
    # Assumes synchronized clocks! âŒ

# âœ… CORRECT: Use logical clocks (Lamport timestamps)

def is_causally_after(event1, event2):
    return event1.logical_clock > event2.logical_clock
    # Doesn't depend on physical time âœ…
```

**Timestamp Ordering Example**:

```
Real Order:
  T0: Server A: User clicks "post" (time: 10:00:00.100)
  T1: Server B: User sees post (time: 10:00:00.050)

Wait, what? Effect before cause!?

Reason: Server B's clock is 50ms behind Server A's clock

Result: Posts appear out of order in timeline!
```

**Solutions**:

1. **Don't rely on clock synchronization for ordering**
   - Use sequence numbers
   - Use logical clocks (Lamport timestamps, vector clocks)

2. **Use Google TrueTime-like confidence intervals**
   ```python
   # Instead of: timestamp = 10:00:00.123
   # Use: timestamp = [10:00:00.123 Â± 5ms]
   # 
   # Wait until intervals don't overlap before committing
   ```

3. **Combine with other mechanisms**
   - Clocks + version numbers
   - Clocks + consensus algorithms

### Process Pauses

Even if networks and clocks were perfect, processes can pause unexpectedly!

**Causes of Process Pauses**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHY PROCESSES PAUSE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  1. Garbage Collection (GC)                    â”‚
â”‚     - "Stop the world" GC pause                â”‚
â”‚     - Can be hundreds of milliseconds          â”‚
â”‚     - All threads frozen!                      â”‚
â”‚                                                â”‚
â”‚  2. Virtual Machine Suspension                 â”‚
â”‚     - Hypervisor pauses VM                     â”‚
â”‚     - Seconds or even minutes!                 â”‚
â”‚     - VM doesn't know it was paused            â”‚
â”‚                                                â”‚
â”‚  3. Operating System Context Switching         â”‚
â”‚     - Laptop closed (sleep mode)               â”‚
â”‚     - Process swapped to disk                  â”‚
â”‚                                                â”‚
â”‚  4. Synchronous Disk I/O                       â”‚
â”‚     - Page fault, need to load from disk       â”‚
â”‚     - Slow HDD can pause for 100ms             â”‚
â”‚                                                â”‚
â”‚  5. Other Processes                            â”‚
â”‚     - CPU contention                           â”‚
â”‚     - Another process using all CPU            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real-World Example - GC Pause**:

```java
// Java application

public class LeaderElection {
    private long lastHeartbeat;
    private boolean isLeader = false;
    
    public void runLeaderHeartbeat() {
        while (isLeader) {
            sendHeartbeat();
            lastHeartbeat = System.currentTimeMillis();
            
            // GC PAUSE HERE for 15 seconds! â¸ï¸
            // (Application frozen, doesn't know!)
            
            Thread.sleep(10000);  // 10 seconds
        }
    }
    
    public void checkLeaderAlive() {
        long now = System.currentTimeMillis();
        if (now - lastHeartbeat > 20000) {  // 20 second timeout
            // Leader hasn't sent heartbeat!
            // Declare leader dead, start election
            startElection();
        }
    }
}

// What happens:
// T0: Leader sends heartbeat
// T1: GC pause for 15 seconds â¸ï¸
// T2: Other nodes: "Leader dead!" (15s > timeout)
// T3: New leader elected
// T4: GC finishes, old leader resumes
// T5: TWO LEADERS! ğŸ’¥ğŸ’¥ (split-brain)
```

**Protecting Against Process Pauses**:

```python
# Solution: Fencing tokens

class LeaderWithFencing:
    def __init__(self):
        self.token = 0  # Monotonically increasing token
    
    def become_leader(self):
        # Get token from coordination service (ZooKeeper, etcd)
        self.token = coordination_service.get_next_token()
        # token = 42
        
    def write_data(self, data):
        # Include token with every write
        storage.write(data, fencing_token=self.token)
        
# Storage system:
class Storage:
    def __init__(self):
        self.current_token = 0
    
    def write(self, data, fencing_token):
        if fencing_token < self.current_token:
            # Old leader trying to write!
            raise FencingTokenTooOld()
        
        self.current_token = fencing_token
        self._write(data)

# Timeline:
# T0: Leader A gets token=42
# T1: Leader A writes with token=42 âœ…
# T2: Leader A pauses (GC)
# T3: New leader B elected, gets token=43
# T4: Leader B writes with token=43 âœ… (storage accepts, updates current_token=43)
# T5: Leader A resumes, tries to write with token=42
# T6: Storage rejects! (42 < 43) âŒ
```

## Part 4: Knowledge, Truth, and Lies

In distributed systems, we face philosophical questions!

### The Truth is Defined by the Majority

**Question**: How do you know if a node is alive or dead?

**Answer**: You can't know for sure. You can only believe based on messages you receive.

**Scenario - Is Node Dead or Just Slow?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NODE FAILURE DETECTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  [Node A] sends heartbeats                     â”‚
â”‚      â†“                                         â”‚
â”‚  [Node B] receives heartbeats                  â”‚
â”‚      â†“                                         â”‚
â”‚  Time passes... no heartbeat                   â”‚
â”‚      â†“                                         â”‚
â”‚  Question: Is A dead or just slow?             â”‚
â”‚                                                â”‚
â”‚  Possibility 1: A crashed ğŸ’¥                   â”‚
â”‚  Possibility 2: Network problem ğŸŒâŒ            â”‚
â”‚  Possibility 3: A is slow ğŸŒ                   â”‚
â”‚  Possibility 4: B is slow ğŸŒ                   â”‚
â”‚                                                â”‚
â”‚  B cannot distinguish!                         â”‚
â”‚                                                â”‚
â”‚  Solution: Use quorum (majority vote)          â”‚
â”‚                                                â”‚
â”‚  [Node B] "A is dead"                          â”‚
â”‚  [Node C] "A is dead"                          â”‚
â”‚  [Node D] "A is alive"                         â”‚
â”‚      â†“                                         â”‚
â”‚  Majority (2/3) says dead â†’ Declare A dead     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Quorum-Based Consensus**:

```python
def is_node_alive(node_id, quorum_size=3):
    votes = []
    
    for checker in cluster_nodes:
        if checker == node_id:
            continue
        
        # Ask if node responds
        if checker.can_reach(node_id):
            votes.append(True)
        else:
            votes.append(False)
    
    # Majority vote
    alive_votes = sum(votes)
    dead_votes = len(votes) - alive_votes
    
    if alive_votes >= quorum_size // 2 + 1:
        return True  # Majority says alive
    else:
        return False  # Majority says dead
```

### Byzantine Faults

**Byzantine Fault**: Node behaves maliciously or arbitrarily (not just failing silently).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BYZANTINE vs NON-BYZANTINE FAULTS             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Non-Byzantine (Crash fault):                  â”‚
â”‚    Node works correctly OR crashes             â”‚
â”‚    [Node] âœ… â†’ works                           â”‚
â”‚    [Node] ğŸ’¥ â†’ crashed (silent)                â”‚
â”‚                                                â”‚
â”‚  Byzantine:                                    â”‚
â”‚    Node may send wrong/malicious messages      â”‚
â”‚    [Node] ğŸ˜ˆ â†’ sends different messages to     â”‚
â”‚                  different nodes                â”‚
â”‚    [Node] ğŸ˜ˆ â†’ claims it received message      â”‚
â”‚                  it didn't receive              â”‚
â”‚    [Node] ğŸ˜ˆ â†’ corrupts data                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Examples of Byzantine Faults**:

1. **Malicious Attacker**
   - Hacker compromises node
   - Sends false information

2. **Hardware Corruption**
   - Cosmic ray flips bit in memory
   - CPU calculates wrong result

3. **Software Bug**
   - Nondeterministic behavior
   - Different results on different runs

**Byzantine Fault Tolerance (BFT)**:

Systems that tolerate Byzantine faults are **much more complex** and **much slower**.

```python
# Byzantine consensus (simplified)
# Requires 3f + 1 nodes to tolerate f Byzantine nodes

def byzantine_consensus(nodes, value):
    # Need 2f + 1 agreeing messages to commit
    # (f Byzantine nodes can't forge majority)
    
    messages = []
    for node in nodes:
        msg = node.broadcast(value)
        messages.append(msg)
    
    # Count votes
    vote_counts = {}
    for msg in messages:
        vote_counts[msg.value] = vote_counts.get(msg.value, 0) + 1
    
    # Need supermajority (2f + 1)
    for value, count in vote_counts.items():
        if count >= 2 * max_byzantine_nodes + 1:
            return value
    
    # No consensus
    return None
```

**Real-World Usage**:
- **Blockchain**: Bitcoin, Ethereum (Byzantine-tolerant)
- **Most databases**: NOT Byzantine-tolerant (assumes non-malicious failures)

**Why Not Always Use BFT?**
- Much slower (3x+ latency)
- More complex
- Requires more nodes (3f+1 vs 2f+1)
- Most systems assume trusted internal network

### System Models

To reason about distributed systems, we define models that describe what can go wrong.

**Timing Models**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYSTEM TIMING MODELS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Synchronous:                                  â”‚
â”‚    - Bounded network delay                     â”‚
â”‚    - Bounded process pause                     â”‚
â”‚    - Bounded clock error                       â”‚
â”‚    Reality: Aircraft systems, hard real-time   â”‚
â”‚                                                â”‚
â”‚  Partially Synchronous (MOST COMMON):          â”‚
â”‚    - Usually behaves like synchronous          â”‚
â”‚    - Sometimes delays unbounded                â”‚
â”‚    Reality: Most data centers, cloud systems   â”‚
â”‚                                                â”‚
â”‚  Asynchronous:                                 â”‚
â”‚    - No timing assumptions at all              â”‚
â”‚    - Arbitrarily long delays                   â”‚
â”‚    Reality: Theoretical model, pessimistic     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Node Failure Models**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NODE FAILURE MODELS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Crash-stop:                                   â”‚
â”‚    - Node works correctly OR crashes           â”‚
â”‚    - Doesn't recover after crash               â”‚
â”‚                                                â”‚
â”‚  Crash-recovery (MOST COMMON):                 â”‚
â”‚    - Node may crash at any time                â”‚
â”‚    - May recover after crash                   â”‚
â”‚    - Stable storage survives crash             â”‚
â”‚                                                â”‚
â”‚  Byzantine:                                    â”‚
â”‚    - Node may behave arbitrarily               â”‚
â”‚    - May send wrong/malicious messages         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Most real systems assume**: Partially synchronous + Crash-recovery model

### Safety and Liveness

**Two types of properties**:

**Safety**: "Nothing bad happens"
- Examples:
  - No data loss
  - No data corruption
  - Uniqueness (no duplicate IDs)
- Must ALWAYS be true
- If violated once, can't be undone

**Liveness**: "Something good eventually happens"
- Examples:
  - Request eventually completes
  - Node eventually responds
- Allows delays
- Can retry

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SAFETY vs LIVENESS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Safety:                                       â”‚
â”‚    "The system will never return wrong data"   â”‚
â”‚    Must be true at ALL times                   â”‚
â”‚    If violated â†’ permanent damage              â”‚
â”‚                                                â”‚
â”‚  Liveness:                                     â”‚
â”‚    "The system will eventually respond"        â”‚
â”‚    May take time, but will happen              â”‚
â”‚    If violated â†’ retries may help              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Trade-off**: In partially synchronous systems with crash-recovery nodes:
- Can guarantee **safety**
- Can guarantee **liveness** (with caveats)
- But difficult to guarantee both simultaneously!

## Summary

**Key Takeaways**:

1. **Partial Failures are Fundamental**
   - Can't avoid them in distributed systems
   - Must design for them from the start

2. **Networks are Unreliable**
   - Packets lost, delayed, duplicated
   - Timeouts are necessary but imperfect
   - No guarantees on delivery time

3. **Clocks are Unreliable**
   - Clocks drift apart
   - NTP synchronization is imperfect
   - Don't rely on synchronized clocks for ordering
   - Process pauses can cause stale data

4. **Truth is Subjective**
   - No node has complete information
   - Majority vote (quorum) determines truth
   - Byzantine faults are rare but serious

5. **System Models Help Reasoning**
   - Partially synchronous + crash-recovery most common
   - Safety vs liveness properties
   - Different guarantees require different assumptions

**Design Principles**:

```
âœ… Assume networks will fail
âœ… Use timeouts, but choose them carefully
âœ… Don't depend on synchronized clocks for correctness
âœ… Use logical clocks (Lamport timestamps) for ordering
âœ… Use quorums and consensus for important decisions
âœ… Design for crash-recovery (stable storage)
âœ… Assume processes can pause unexpectedly
âœ… Use fencing tokens to prevent split-brain
```

**Next Chapter**: Consistency and Consensus - how to build reliable distributed systems despite all these problems!
